---
title: "DATA698 Project - Vehicle Loan Default Prediction"
author: "Chinedu Onyeka"
date: "12/09/2023"
output:
  html_document:
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
    code_folding: "hide"
  pdf_document: default
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<center> <h2> 0. ABSTRACT </h2> </center>  
This project aims to develop a machine learning model to predict auto-loans default using Machine Learning techniques. The dataset used was gotten from kaggle and contains about 41 variables and over 230,000 records of previous loan applicants.  
Most banks typically use Logistic Regression Model to make decision whether an applicant is at risk of default or not. However, in this project, several other machine learning techniques were explored such as Tree-based models like Random Forest, and XGBoost (Extreme Gradient Boosting) as well as Artificial Neural Networks. The Logistic Regression Models was developed as well and compared to these other models. We found that the Neural Network model provided the best result based on the recall. Also, the logistic regression model does not perform poorly compared to other models, but instead provided competitive performance as well.  However, due to time and resources constraints, the models were not fully exploited to get the best out of each of the models. Simply, hyper-parameter tuning was not done on the models that have tunable parameters, and feature engineering was not done. Ultimately, the Logistic Regression Model was chosen based on its competitive overall performance good enough recall, accuracy, auc value when compared to the other three models. However, even though the Neural Network (NNET) model has better recall, it may be difficult to interpret the model to determine why a customer was refused a loan. Hence, the Logistic Regression Model which is the industry standard is still chosen here because of its simplicity, ease of understanding and interpretation, and very short training time.   

<center> <h2> 1. INTRODUCTION </h2> </center> 
<h3> 1.1 Research Problem </h3>  
The problem that this project aims to solve is to develop a machine learning model that will predict whether a potential borrower will default on their auto-loan or not using available data collected about the customer.  
The auto-loan industry is a multi-billion dollar industry that affects almost all aspects of life in the developed world as well as in developing countries. Automobiles are now the de facto means of movement in suburbs and even in some major cities and the volume of automobiles on the roads has increased tremendously over the past decades. Buying an automobile (referred to in this project as a vehicle) is an important decision for most people and banks are usually at the center of this decision since majority of vehicle purchases are made through financing option. Hence, financial institutions like banks have to face this particular problem all the time to decide whether a borrower will be able to make payments throughout the lifetime of their auto-loan.  
Hence, banks do not want to issue loans to individuals that would default and at the same time do not want to deny loans for individuals that would not default. To do this, they want to be able to predict to a high level of confidence whether to approve or deny an auto-loan so that they will be able to minimize losses and write-offs when a borrower is not able to make payments and at the same time make profits from customers who are able to service their loans. This project aims to solve this problem by developing a predictive model using machine learning techniques to help banks decide which potential borrower is likely to default on their loans or not.  
</br>  

<h3> 1.2 Definition of Terms </h3>  
**Automobile: ** "Automobile, byname auto, also called motorcar or car, a usually four-wheeled vehicle designed primarily for passenger transportation and commonly propelled by an internal-combustion engine using a volatile fuel" (Cromer et al., 2023). Automobiles could also be referred to as a vehicle, motor vehicle, light trucks, etc. These days, automobiles can be propelled by either an internal-combustion engine or by electric battery. About 282 million vehicles were registered in the United States in 2021 about 90 million more compared to about 193 million in 1990. (U.S. Vehicle Fleet 1990-2021 | Statista, 2023). Also, the average price of motor vehicles in the United States in 2022 is about $46,000 USD  (U.S.: Average Selling Price of New Vehicles 2022 | Statista, 2023)  

**Auto Loan: ** An auto loan is the money you borrow to pay for your car. You will have to repay the loan with interest in fixed installments (Martin, 2023). Auto Loans are also referred to as car loans, vehicle loans, car financing, etc. These loans are often secured loans meaning that the car is used as the collateral to secure the loan. Typically, consumers borrow money to buy vehicles. In fact, consumers owed about 1.41 trillion US dollars on vehicles they drove in 2022. Also, the average auto loan balance is about $22,000 USD. In addition, about 80% of all new vehicles on the road is financed through a loan or lease (Chris, 2023). This goes to tell us that the auto-loan industry is a multi-trillion dollar industry which signifies the importance of loan default models to help minimize losses.  

**Vehicle Loan Default: ** Loan default refers to when a borrower fails to make their installment payments as agreed by the loan terms. Usually for secured loans, the lender (bank in this case) can repossess the asset (car) which is used as collateral for the loan. Banks usually do not want to do that but sometimes are forced to do that if the borrower defaults and does not make an arrangement with the lender. When a lender repossess the car, the value of the car at the time of repossession may not be up to the loan balance and the lender will have no option than to write off that balance as a loss. In the US, default rates for auto loans are on the rise and currently sits at about 2%. The benefits of being able to predict whether a borrower will default cannot be over emphasized as it will not only help lenders to know whether to approve or deny a loan application, it will also help them price the interest rate for lenders appropriately.  
The problem of loan default is in all ramifications very probabilistic and it's often also considered strongly in credit risk modeling/credit scoring. Hence, lenders use a vast amount of credit risk tool to determine whether a borrower is likely to default or not. Entering default simply means that the lender determines that the borrower is not going to pay, usually some time after 90 days of no payments — can translate into your car being repossessed (O’Brien, 2023).  
</br>

* * *
<center> <h2> 2. LITERATURE REVIEW </h2> </center>  
The probability of loan default is a well researched topic especially because the industry is a multi-trillion dollar industry. This area is of significant economic importance and often also regarded as credit risk modeling or credit scoring. We review literature related to credit in general and then those related to auto-loan.  

In Crook et. al.(2007), Credit scoring is concerned with developing empirical models to support decision making in the retail credit business. Also, a credit score is a model-based estimate of the probability that a borrower will show some undesirable behavior in the future. In application scoring, for example, lenders employ predictive models, called scorecards, to estimate how likely an applicant is to default. Such PD (probability of default) scorecards are routinely developed using classification algorithms (Hand & Henley, 1997).  

Whilst the extension of credit goes back to the Babylonian times, the history of credit scoring began in 1941 with the publication by Durand of a study that distinguished between good and bad loans made by 37 firms (Crook et al. 2007). Since then, the already established techniques of statistical discrimination have been developed and an enormous number of new classification algorithms have been researched and tested. Virtually all major banks use credit scoring with specialized consultancies providing credit scoring services and offering powerful software to score applicants, monitor their performance and manage their accounts.  

Altman and Saunders (1998) published an overview of credit risk modelling for the last 20 years. They found that credit risk modelling has evolved drastically for the past 20 years due to new emerging statistical techniques(Altman & Saunders, 1998). Later, another group of researchers published an extension to Altman and Saunders work presenting a further development of credit risk modelling(Hao, Alam, & Carling, 2010). Their work identified more than 1000 articles on this topic, and found that logistic regression (LR) model and discriminant analysis are the most widely used methods for constructing scoring systems.  

Also, Crook et al.(2007) conducted a research on credit risk scoring and found that the commonest method of estimating a classifier of applicants into those likely to repay and those unlikely to repay is logistic regression with the logit value compared with a cut off. Basically, this research makes claim that the industry standard for predicting loan default is the logistic regression model.  

Lessmann et al.(2015) compared about 41 classifiers based on six performance measures across eight real-world credit scoring data sets from the UK, Europe, and Australia. They investigated the overall model performance using several datasets, and examine the predictive performance in each case. The conclusion from this research suggests that several classifiers predict risk significantly better than the industry standard of Logistic Regression (LR). It went further to recommend the Random Forests(RF) model as a benchmark model because of its effectiveness, precision, and its interpretability.  

Agrawal et al.(2014) studied the impact of contract-specific variables as predictors in commercial vehicle loans. In their research, applying a logistic regression model for predicting default, around 11 out of 17 contract-specific variables where identified to provide additional assistance for the credit lending institution(Agrawal, Agrawal, & Raizada, 2014). The authors also suggest that contract information could improve the accuracy in more advanced nonlinear models. Specifically, the authors suggest the use of Neural Networks as one potential predictive model to improve the performance based on contract information(Agrawal et al.,2014).  

Keeping the outcome of the above literature in mind, this project aims to contribute to the field of vehicle loan prediction by developing machine learning models that will be able to predict vehicle loan default based on the available data, and  also compare  three (3) models: Random Forest, XGBoost, and Neural Networks models to the industry standard Logistic Regression model. The data set used in this project contains more data than those used in most of the literature reviewed above. Statistically, more data provides better results and we hope that will be useful in better comparing the model and decide which model provides the best prediction metrics. In this work, we explain the data used, the pre-processing and feature selection involved and also provide a quick overview of predictive analytics as well as quick review to help understand the scoring metrics for classification problems. In addition, each of the models used are briefly explained and then the analysis of the data followed by modeling/testing and then conclusion on the findings.
</br>

* * *

<center> <h2> 3. OVERVIEW OF THE MODELS </h2> </center> 
<h3> 3.1 Predictive Analytics Overview </h3>  
Predictive analytics involves using historical data, statistical algorithms, and machine learning techniques to predict future outcomes. There are different types of algorithms for making predictions. Generally, we have regression models and classification models. Regression algorithms are mainly used when the variable to be predicted is a continuous value while classification algorithms are used to predict categories or classes.Classification algorithms are a fundamental part of predictive analytics and are used to categorize data into classes or groups based on specific features or attributes. Examples of classification algorithms include but not limited to Logistic Regression, Decision Trees, Random Forest, XGBoost, SVM, KNN, Naive Bayes, etc. The problem of vehicle loan default prediction is a classification problem and the classification algorithms that will be used to solve the problem of loan default in this project are briefly explained below:

<h3> 3.2 Logistic Regression </h3>  
Logistic Regression is a type of classification algorithm that is used when the response variable is binary in nature. Being binary means that the response variable is a two level categorical variable. The Logistic Regression model belongs to the family of generalized linear models (GLMs) and it's used when the response is a two-level categorical variable. Typical examples of binary response variables are Yes/No; Male/Female; Cancer/No Cancer; Approve/Deny, etc. These response variables are often coded as 1 or 0. Even though the name contains regression, the logistic regression model is often used when the response variable is discrete. i.e It is a classification algorithm.  
Logistic Regressions must not necessarily be binary as there are possible situations where there are more than two-level categories in the response variables and such situations are regarded as multinomial logistic regression which is beyond the scope of this project.  
The logistic regression model relates the probability that a response variable would be successful to the predictors $x_{1, i}, x_{2, i}, ..., x_{k, i}$ through a framework like that of multiple regression:  
$logit(p_{i}) = log_{e}(\frac{p_{i}}{1-p_{i}}) = \beta_{0} + \beta_{1}x_{1,i} + \beta_{2}x_{2,i} + ... + \beta_{k}x_{k,i}$  

<b>Assumptions for Logistic Regression</b>  
<li> Each outcome of the response variable is independent of the other outcomes. </li> 
<li> The response variable must follow a binomial distribution. </li> 
<li> Each predictor $x_{i}$ is linearly related to the $logit(p_{i})$ if other predictors are held constant. </li>  
  

<h3> 3.3 Tree-Based Methods </h3>  
Tree-based models are a type of supervised learning algorithm used for both classification and regression problems. They construct a decision tree that recursively splits the data into subsets based on the most significant features. The tree structure consists of nodes representing the features, edges representing the decision rules, and leaves representing the output (class or value). The key advantage of tree-based models is their ability to handle non-linear relationships and interactions between features. However, they are prone to overfitting, especially when the trees become too complex. Tree based models follow two major approaches - bagging or boosting.  
<b>Bagging (Bootstrap Aggregating): </b> It's a technique that aims to reduce variance and prevent overfitting by training multiple models on different bootstrapped subsets of the dataset and then averaging the predictions. Random Forest is an ensemble method based on bagging, utilizing multiple decision trees trained on different subsets of the data.  
<b>Boosting: </b> Boosting is an ensemble technique that combines weak learners (typically shallow trees) sequentially to create a strong model. It focuses on improving the shortcomings of its predecessors by assigning higher weight to misclassified data, effectively learning from previous mistakes. Gradient Boosting and XGBoost are examples of boosting algorithms.  

<h4> 3.3.1 Random Forest </h4>  
It's an ensemble learning method that constructs multiple decision trees and merges their predictions to improve accuracy and reduce overfitting. It introduces randomness both in feature selection and dataset bootstrapping. By aggregating predictions from various trees, it tends to be more robust and less prone to overfitting compared to a single decision tree.  

<h4> 3.3.2 Gradient Boosting </h4>  
This is a boosting technique that builds trees sequentially. It fits each tree to the residuals (errors) of the preceding tree, reducing the errors at each step. Gradient boosting is a powerful algorithm known for its ability to handle complex data and achieve high accuracy.  

<h4> 3.3.2 XGBoost (Extreme Gradient Boosting) </h4>  
It is an optimized and highly efficient implementation of gradient boosting. XGBoost improves upon the traditional gradient boosting method by introducing regularization, parallel computing, and a variety of enhancements that significantly speed up the training process and improve accuracy.

<h3> 3.4 Neural Networks </h3>  
Artificial Neural Networks (ANNs) are a class of machine learning models inspired by the structure and function of the human brain. They consist of interconnected nodes, known as neurons, organized in layers to process and learn from data.  

<h5>Structure of ANNs </h5>  
<li><b>Neurons(Nodes):</b> Neurons are the basic units in an ANN. Each neuron receives input signals, performs computations, and produces an output signal that is transmitted to other neurons in the network. </li> 
<li><b>Layers: </b> ANNs typically consist of three types of layers:  
*Input Layer:* Receives input data and passes it to the next layer.  
*Hidden Layers:* Intermediate layers between the input and output layers. They extract and transform features through complex computations.  
*Output Layer:* Produces the final output or prediction based on the learned representations from the hidden layers. </li>
<li><b>Connections (Weights):</b> Neurons are connected to neurons in adjacent layers by weighted connections. These weights are adjusted during the learning process to optimize the network's performance.  </li>

<h5>Training Process</h5>  
<li><b>Forward Propagation:</b> Input data is passed through the network, and computations are performed layer by layer, generating an output. </li>
<li><b>Loss Calculation:</b> The output is compared to the actual target, and a loss function measures the difference between the predicted and actual values. </li> 
<li><b>Backpropagation:</b> The algorithm calculates gradients of the loss function with respect to the weights of the network using chain rule and propagates this information backward through the network. </li>  
<li><b>Weight Update:</b> The weights are adjusted using optimization algorithms (e.g., gradient descent) to minimize the loss function, making the predictions closer to the actual values.  </li>

<h5>Types of ANNs</h5>  
<li><b>Feedforward Neural Networks (FNN):</b> The most basic type where information flows in one direction—from input to output without cycles. This is the type used in this project.</li>
<li><b>Recurrent Neural Networks (RNN):</b> Networks that allow feedback loops, enabling them to process sequential data by retaining memory of previous inputs.</li>  
<li><b>Convolutional Neural Networks (CNN):</b> Specialized for processing grid-like data, such as images. They use convolutional layers to learn features hierarchically.  </li>

<h5>Applications</h5>  
*Image and Speech Recognition:* CNNs are widely used in image classification, object detection, and speech recognition.  
*Natural Language Processing (NLP):* RNNs and variants like LSTM and GRU are used for text analysis, language translation, and sentiment analysis.  
*Predictive Modeling:* ANNs are applied in various predictive tasks, including regression, classification, and time-series forecasting.  

<h5>Challenges</h5>
*Computational Complexity:* Training large ANNs can be computationally intensive and require significant resources.  
*Overfitting:* ANNs, especially with complex architectures, can overfit the training data if not properly regularized or trained on diverse data.  


<h3> 3.5 Scoring the Model and Understanding the Scoring Metrics </h3>  
After obtaining and fitting a classification model, we need certain metrics to evaluate how well the model performs.Below are some metrics that can be used to evaluate the performance of a classification model:  
<center>
<img src="https://github.com/chinedu2301/data621-business-analytics-data-mining/blob/main/blogs/confusion-matrix.png?raw=true" />  
source: https://www.debadityachakravorty.com/ai-ml/cmatrix/
</center>  
<br>
The type of metrics to be used depends on the situation.
<li> **Accuracy: **  This is the most common measure used to evaluate a classification model. Accuracy is the ratio of correctly classified observations to the total. This tells us the percentage of observations that our model is correctly classifying.   
$accuracy = \frac{TP + TN}{TP+FP+TN+FN}$  
Accuracy is great for symmetric datasets and when the cost of false positives and false negatives are similar.</li>  
<li> **Precision: ** This is the percentage of the results that are relevant. It is computed by calculating the number of true positives (TP) divided by all positively classified observations by the model (both TP and FP). The precision tells us the percentage of observations that are actually positive from all positively classified observations by the model. i.e a precision of 90% means that of all the observations that are classified as positive by our model, only 90% of those are actually positive which means that the model correctly classified the positive cases in 90% of the cases.  
$precision = \frac{TP}{TP+FP}$  
We use precision when we want to be more confident about our true positives. For example, in spam/ham emails, you want to be sure that the email is spam before putting it in the spam box. </li> 
<li> **Recall (Sensitivity): ** The recall is also regarded as the sensitivity of the model. This refers to the percentage of total relevant results that are correctly classified by the model. Essentially, it means the ratio of the number of positively classified observations to the total number of actual positives both those correctly classified and incorrectly classified by the model (TP and FN). It is computed by dividing the number of true positives by the total number of observations that are actually positive (whether correctly classified by the model or not). The Recall also known as Sensitivity tells us what proportion of the positive class got correctly classified. i.e What percentage of cancer patients got correctly classified as cancer patients by the model. A recall of 95% means that of all the actually positive cases, only 95% were correctly classified by the model.  
$recall = \frac{TP}{TP+FN}$  
We use recall when having a false positive is way better than having a false negative. for example, you want to tell someone that they have cancer (FP) when in fact they don't instead of telling them that they don't have cancer(FN) when in fact they have it. It would be disastrous to give a False Negative to a cancer patient because they would probably have had time to ameliorate the situation if they had been informed earlier. The recall is better when the cost of false negatives is unacceptable. i.e. False positive is better than false negative.</li> 
<li> **F1 Score: ** This is the harmonic mean of the precision and recall. It is also called F-score or F-Measure.  
$F1 = \frac{2 * precision * recall}{precision + recall}$  
F1 is best for uneven distribution and it can be used to compare different models.</li>  


<h3> 3.6 Underfitting and Overfitting </h3>  
In supervised machine learning problems, we aim to get a model that properly fits the training data and also generalizes well on new/test data. When the model is unable to generalize on new data, it is not able to perform its purpose. One of such causes could be as a result of underfitting or overffiting.  
<li> **Undefitting: ** This is a situation in machine learning where a model does not properly fit the training data resulting in high training error and high test error. In this case, the model performs poorly on the training data as well as on new data.  An underfit machine learning model is not a suitable model for the data because it is not able to properly capture the relationship between the input examples (X) and the target values (Y).Poor performance on the training data could mean that the model is too simple to describe the target properly. A typical example of an underfit model is using a linear model for data points with quadratic relationship.  
Since the model cannot generalize well on new data, it cannot be leveraged for prediction or classification tasks. High bias and low variance are good indicators of underfitting. </li>
<li> **Overfitting: ** This is simply the opposite of underfitting. It is a situation in machine learning where a model properly fits the training data, but performs poorly on new datasets thereby resulting in low training error, but high test error. It involves good performance on training data, but poor performance on new/test data. The model is essentially learning the noise and details in the training data and memorizing it such that it can not generalize to unseen data. A typical example of overfitting is fitting a quadratic function with a cubic or higher order polynomial model. High variance and low bias are indicators of overfitting. </li>  
<br>

<center>
<img src = "https://github.com/chinedu2301/data621-business-analytics-data-mining/blob/main/blogs/underfitting-overfitting-aws.png?raw=true" />
source: https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html
</center>
<br>

<center> <b> Reducing Underfitting  </b> </center>  
There are different ways to decrease underfitting such are:
<li>Increase model complexity. It could be that the selected model is too simple and a more complex model may be required.</li>
<li>Perform feature selection/feature engineering. </li>
<li>Increase the duration of training to get better results.</li>
<li>Decrease the amount of regularization used.</li>

<center> <b> Reducing Overfitting  </b> </center>  

<li>Reduce the model complexity. It could be that the model is too complex and a simpler model is required.</li>
<li>Increase the amount of training data</li>
<li>Perform feature selection: Reduce the number of features</li>
<li>Increase the amount of regularization</li>
<li>Early stopping in the training</li>
<li>Perform cross validation</li>
<li>Use ensemble methods</li>  
</br>  

* * *

<center> <h2> 4. METHODOLOGY </h2> </center> 
<h3> 4.1 Dataset </h3>   
The data used in this analysis was obtained from [Kaggle](https://www.kaggle.com/datasets/avikpaul4u/vehicle-loan-default-prediction/data). The csv files containing the data was downloaded from kaggle and placed in [github](https://github.com/chinedu2301/data698-analytics-project/tree/main/data). There are three files which are: train dataset, test dataset, and dictionary. The dictionary file explains what the variables represent. Unfortunately, the test dataset does not contain labels (i.e the response variable) and could not be used to evaluate how well the model does because to do that, you need to have labels to compare to the predicted values. Hence, After cleaning of the data, the train data was split using the `CaTools` package into training `(df_train)` and testing data `(df_test)` 

<h5>Variables</h5>  
There are about 41 variables in the dataset. forty (40) of the variables are predictor variables while there is one response variable. The variables in the dataset are:  
<li> <b>UniqueID</b>: Identifier for customer </li>
<li> <b>loan_default</b>: Payment default in the first EMI on due date </li>
<li> <b>disbursed_amount</b>: Amount of loan disbursed </li>
<li> <b>asset_cost</b>: Cost of the Asset </li>
<li> <b>ltv</b>: Loan to Value of asset </li>
<li> <b>branch_id</b>: Branch where the loan was disbursed </li>
<li> <b>supplier_id</b>: Vehicle Dealer where the loan was disbursed </li>
<li> <b>manufacturer_id</b>: Vehicle manufacturer (Hero, Honda, TVS, etc.) </li>
<li> <b>Current_pincode</b>: Current pincode of the customer </li>
<li> <b>Date.of.Birth</b>: Date of Birth of the customer</li>
<li> <b>Employment.Type</b>: Employment Type of the customer (Salaried/Self Employed) </li>
<li> <b>DisbursalDate</b>: Date of loan disbursement </li>
<li> <b>State_ID</b>: State of disbursement</li>
<li> <b>MobileNo_Avl_Flag</b>: If Mobile no. was shared by the customer then flagged as 1 </li>
<li> <b>Aadhar</b>: If aadhar was shared by the customer then flagged as 1 </li>
<li> <b>PAN_flag</b>: If pan was shared by the customer then flagged as 1</li>
<li> <b>VoterID_flag</b>: If voter Id was shared by the customer then flagged as 1</li>
<li> <b>Driving_flag</b>: If Driver license was shared by the customer then flagged as 1 </li>
<li> <b>Passport_flag</b>: If passport was shared by the customer then flagged as 1 </li>
<li> <b>PERFORM_CNS.SCORE</b>: Bureau Score</li>
<li> <b>PERFORM_CNS.SCORE.DESCRIPTION</b>: Bureau Score description</li>
<li> <b>PRI.NO.OF.ACCTS</b>: Count of total loans taken by the customer at the time of disbursement</li>
<li> <b>PRI.ACTIVE.ACCTS</b>: Count of active loans taken by the customer at the time of disbursement</li>
<li> <b>PRI.OVERDUE.ACCTS</b>: Count of default accounts at the time of disbursement</li>
<li> <b>PRI.CURRENT.BALANCE</b>: Total principal outstanding amount of the active loans at the time of disbursement</li>
<li> <b>PRI.SANCTIONED.AMOUNT</b>: Total amount that was sanctioned for all the loans at the time of disbursement</li>
<li> <b>PRI.DISBURSED.AMOUNT</b>: Total amount that was disbursed for all the loans at the time of disbursement</li>
<li> <b>SEC.NO.OF.ACCTS</b>: Count of total loans taken by the customer at the time of disbursement</li>
<li> <b>SEC.ACTIVE.ACCTS</b>: Count of active loans taken by the customer at the time of disbursement</li>
<li> <b>SEC.OVERDUE.ACCTS</b>: Count of default accounts at the time of disbursement</li>
<li> <b>SEC.CURRENT.BALANCE</b>: Total principal outstanding amount of the active loans at the time of disbursement</li>
<li> <b>SEC.SANCTIONED.AMOUNT</b>: Total amount that was sanctioned for all the loans at the time of disbursement</li>
<li> <b>SEC.DISBURSED.AMOUNT</b>: Total amount that was disbursed for all the loans at the time of disbursement</li>
<li> <b>PRIMARY.INSTAL.AMT</b>: EMI Amount of the primary loan</li>
<li> <b>SEC.INSTAL.AMT</b>: EMI Amount of the secondary loan</li>
<li> <b>NEW.ACCTS.IN.LAST.SIX.MONTHS</b>: New loans taken by the customer in the last 6 months before the disbursement</li>
<li> <b>DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS</b>: Loans defaulted in the last 6 months</li>
<li> <b>AVERAGE.ACCT.AGE</b>: Average loan tenure</li>
<li> <b>CREDIT.HISTORY.LENGTH</b>: Time since first loan</li>
<li> <b>NO.OF_INQUIRIES</b>: Enquiries done by the customer for loans</li>  

Note: Primary accounts are those which the customer has taken for his personal use while secondary accounts are those which the customer act as a co-applicant or guarantor.  
The response variable is the <b>loan_default</b> variable and it is a two level categorical variable with a value of 1 for default and value 0 for no default.  

<h3> 4.2 Methodology </h3>  
The method adopted in this analysis follows a certain number of steps and code re-use was essential to avoid errors resulting from code duplication. To perform this analysis, the following steps were taken in R studio:  

<li>
<b>Read Data:</b> The data was read into memory from the external location and a few records were displayed.
</li>  

<li>
<b>Clean Data:</b> The data was cleaned using the `data_cleaning` function developed in this analysis. It leverages the tidyverse group of packages.  
</li> 

<li>
<b>Explore Data:</b> The data was explored. It was determined that there are about 41 variables (40 predictors and 1 response variable). Also, there were over 230,000 observations and the data was found to be unbalanced after visualizing the two level categorical response variable `loan_default` in a bar chart. Further exploratory data analysis was done to better understand the data.
</li>  

<li>
<b>Pre-Process Data:</b> The data was pre-processed using the `data_preprocessing` function also developed in this analysis.
</li>  

<li>
<b>Train Test Split:</b> Split the data into training and test data.
</li>  

<li>
<b>Model Training:</b> After cleaning, and pre-processing, the train data was used to train four (4) different machine learning models: Logistic, Random Forests, XGBoost, and Neural Network models. These training were done using the `model_training` function developed as part of this analysis as well.
</li> 

<li>
<b>Model Prediction and Evaluation:</b> After training the data and getting a trained model, predictions were made and results of the prediction were compared to the labels in the data to determine how well the model performs using four different metrics: Accuracy, Precision, Recall, and AUC.
</li>  

<li>
<b>Results:</b> The results of the model evaluations were compared to determine the best performing model.
</li>  

<li>
<b>Next Steps and Conclusion: </b> Based on the results of the evaluation metrics and other findings during the analysis, next steps recommendations were made and a conclusion provided on the project findings.
</li>   

Further details about the functions used in the analysis are provided in the section below:  

<h3> 4.3 Data Cleaning and Pre-processing Details </h3>  
To clean and pre-process the data, two major functions `data_training` and `data_preprocessing` were developed respectively to avoid code duplication and errors such that new data can simply be passed into those functions to be cleaned, pre-processed and ready for either training or testing.  

<h4>4.3.1 data_cleaning:</h4>  
This function basically cleans the data. It accepts an R dataframe or a tibble with fixed schema and does the following cleaning:  
<li>
It starts by removing the 'period' in the column names and then converting all the names to snake case (lower) format.  
</li>

<li> 
After which it derives the age of applicants by subtracting the date of birth from the date of disbursement rounded to the nearest whole number after both dates have been converted to date from their string formats using the lubridate package in R.  
</li>

<li>
Then, it normalizes the average_account_age by combining their year and month components and round the result to the nearest year.  
</li>

<li>
Normalized the average_credit_history by combining their year and month components and round the result to the nearest year. 
</li>

<li>
Cleaned up the perform_cns_score_description into meaningful risk categories.  
</li>

<li>
Cleaned up the employment_type of applicant into self_employed, employed, and not_reported.
</li>

<li>
Finally, it selects the relevant columns that will be used further in the analysis and returns the cleaned data as an R dataframe.  
</li>

<h4>4.3.2 data_preprocessing:</h4>  
This function pre-processes the data for either training or testing purposes. It expects the cleaned data from the data_cleaning function and a mode to specify whether the data will be used for training or testing. It has a further helper function `data_preprocess_scaling` that uses the standard-scaler approach to scale the data. For testing data, mode is passed as 'test' and the function basically calls the helper function to scale the data and returned the scaled data that can then be used for testing purposes while for training data, mode is passes as 'train' and the function uses the ROSE package in R to do oversampling/under-sampling to balance the class of data in this binary classification problem after which it calls the helper function to standardize/scale the balanced data.  

<h4>4.3.3 model_training:</h4>  
This function does the model training of any of the four (4) models. It expects a pre-processed dataframe and mode of training. It has a further helper function `xgb_nnet_preprocess` that does further preprocessing for XGB, and Neural Network models. For the XGB model, the train function expects a certain DMatrix which can only be numeric values and in that case, the model_training function calls this helper function which converts the non-numeric predictor variables to dummies, and converts the dataframe into a DMatrix format that can be used to train the data using the XGB algorithm. Also, for the NNET model, the helper function only dummifies the non-numeric predictor variables.  
For models that need further pre-processing, the function calls the appropriate helper function to do that and then trains the model using the data and type of model provided to return a trained model.  

<h4>4.3.4 model_prediction:</h4>  
This behaves similarly like the model_training function although it does so for predictions. It expects three parameters - data, trained_model, and model_type. Depending on the type of the model, it determines if the data needs further pre-processing like in the case of XGB and NNET models whereby it calls the same helper function `xgb_nnet_preprocess` to return the same format of data it gave to the training model as that is what would be needed to make predictions. The predictions returned are probabilities of belonging to a certain class (Class 0). However, this function goes further to convert these probabilities to classes using a given threshold which can always be adjusted based on business needs. For this project, a 50% threshold is used.  

<h4>4.3.5 model_evaluation:</h4>  
After training of a model and using the model to make predictions, it is very important to determine if the predictions of the model can be useful or taken seriously. Hence, we evaluate the model and this function does the evaluation of the model. It calculates the accuracy, precision, recall, and auc for any one of the four models once the actual and predicted values are provided. It relies heavily on the `Metrics` package in R for the evaluations except for auc where it uses the `pROC` package.  

Note: Although, there is a section for exploratory data analysis, there is no function that does this.
</br>  

* * *  

<center> <h2> 5. ANALYSIS, TESTING AND RESULTS </h2> </center> 
<h3> 5.1 Analysis </h3>  
<h4> 5.1.1 Data Cleaning and Pre-processing </h4>  

**Libraries**  
Load the required libraries for the analysis  
```{r load-libraries, warning=FALSE, message=FALSE}
library(Amelia) # To visualize missing data
library(caret)
library(caTools) # for train test split
library(corrplot) # To plot correlation plot
library(cowplot) # To combine plots in a grid
library(fastDummies) # to convert character variables to dummies
library(ggcorrplot) # To plot correlation plot
library(kableExtra)
library(Metrics) # for model evaluation
library(nnet)
library(pROC)
library(ranger) # for random forest implementation
library(ROSE) # to balance the data
library(tidyverse) 
library(xgboost)
```
  

Read the datasets into memory from the github location.
```{r read-data, warning=FALSE, message=FALSE}
url_vehicle_loan_default_train = "https://raw.githubusercontent.com/chinedu2301/data698-analytics-project/main/data/vehicle_default_train_data.csv"
url_vehicle_loan_default_test = "https://raw.githubusercontent.com/chinedu2301/data698-analytics-project/main/data/vehicle_default_test_data.csv"
vehicle_loan_default_train_raw = read_csv(url_vehicle_loan_default_train) %>% as_tibble()
vehicle_loan_default_test_raw = read_csv(url_vehicle_loan_default_test) %>% as_tibble()
```

Display a few records of the dataset to have an idea of how the data looks like.
```{r display-head}
# display a few records of the raw data
raw_data_few_records <- kable(head(vehicle_loan_default_train_raw, 50), "html") %>%
                        kable_paper("hover", full_width = F) %>%
                        scroll_box(width = "850px", height = "350px")
raw_data_few_records
```

<br>
<h4>data_cleaning function</h4>  
The function does cleaning of the data. It only accepts a dataframe (or a tibble) 
```{r data_cleaning_preprocess, warning=FALSE, message=FALSE}
data_cleaning <- function(df){
  # This function accepts a dataframe (df) as input and returns another dataframe (cleaned_df) that is clean.
  tryCatch({
  if(is_tibble(df) | is.data.frame(df)){
    print("The dataframe is a tibble, will proceed to clean data")
    print("Data Cleaning in progress...")
    # rename all the columns in the dataframe to lowercase
    cleaned_df <- df %>% rename_all(tolower) %>%  
                # compute the age of the applicant in number of years
                mutate(date_of_birth = dmy(date_of_birth),  
                      disbursal_date = dmy(disbursal_date), 
                      age = difftime(disbursal_date, date_of_birth, units = "days"),
                      age_years = round(as.numeric(age / 365.25), 0)) %>%
               # extract the years and month component of the average_acct_age  and convert to years
                mutate(average_acct_age_year_comp = as.numeric(str_extract(average_acct_age, "\\d+")),
                      average_acct_age_mon_comp = as.numeric(str_extract(average_acct_age, "\\d+(?=mon)")),
                      average_acct_age = round((average_acct_age_year_comp + average_acct_age_mon_comp/12), 0)
                       ) %>%
                # extract the years and month component of the credit_history_length and convert to years
                mutate(credit_history_length_year_comp = as.numeric(str_extract(credit_history_length, "\\d+")),
                       credit_history_length_comp = as.numeric(str_extract(credit_history_length, "\\d+(?=mon)")),
                       credit_history_length = round((credit_history_length_year_comp + credit_history_length_comp/12), 0)
                       )  %>%
                # clean up the perform_cns_score_distribution to include only a few categories
                mutate(lowercase_cns_description = tolower(perform_cns_score_description),
                       perform_cns_score_description = case_when(
                                str_detect(lowercase_cns_description, "very low risk") ~ "very_low_risk",
                                str_detect(lowercase_cns_description, "low risk") ~ "low_risk",
                                str_detect(lowercase_cns_description, "medium risk") ~ "medium_risk",
                                str_detect(lowercase_cns_description, "high risk") ~ "high_risk",
                                str_detect(lowercase_cns_description, "very high risk") ~ "very_high_risk",
                                str_detect(lowercase_cns_description, "not scored|no bureau") ~ "low_risk",
                                TRUE ~ "none"))  %>%
                # clean up the employment type to have only few categories
                mutate(lower_case_employment_type = tolower(employment_type),
                       employment_type = case_when(
                                str_detect(lower_case_employment_type, "salaried") ~ "salaried",
                                str_detect(lower_case_employment_type, "self employed") ~ "self_employed",
                                TRUE ~ "not_reported")) %>%
                # select only the required columns
                select(
                  age_years, disbursed_amount, asset_cost, ltv, employment_type, perform_cns_score_description,
                  pri_no_of_accts, pri_active_accts, pri_overdue_accts, pri_current_balance, pri_sanctioned_amount,
                  pri_disbursed_amount, sec_no_of_accts, sec_active_accts, sec_overdue_accts, sec_current_balance,
                  sec_sanctioned_amount, sec_disbursed_amount, primary_instal_amt, sec_instal_amt, new_accts_in_last_six_months,
                  delinquent_accts_in_last_six_months, average_acct_age, credit_history_length, no_of_inquiries, loan_default
                )
                print("Data Cleaning complete!!!")
    return(cleaned_df)
  }
  else{
  print("The dataframe is not a tibble. Kindly have your data in the form of a dataframe or a tibble")
  }
    
  },
  #if an error occurs, tell me the error
  error=function(e) {
        message('An Error Occurred')
        print(e)
        },
  #or if a warning occurs, tell me the warning
  warning=function(w) {
        message('A Warning Occurred')
        print(w)
        return(NA)
        }
    )
  
}
```

Use the data_cleaning function to clean the data.

```{r}
# clean the data
vehicle_loan_default_train_cleaned = data_cleaning(vehicle_loan_default_train_raw)
```
Display a few records of the cleaned data.

```{r}
# display a few records of the cleaned data
cleaned_data_few_records <- kable(head(vehicle_loan_default_train_cleaned, 200), "html") %>% 
                            kable_paper("hover", full_width = F) %>%
                            scroll_box(width = "850px", height = "350px")
cleaned_data_few_records
```

<br>
<h4> 5.1.2 Exploratory Data Analysis </h4>
<h5> Check Shape of data</h5>
```{r shape-of-data, warning=FALSE}
dim(vehicle_loan_default_train_cleaned)
```
There are 26 columns (25 predictor variables and 1 response variable) and 233,154 observations.  
<h5> Take a Glimpse at the data </h5>  
```{r glimpse-of-data, warning=FALSE}
glimpse(vehicle_loan_default_train_cleaned)
```

<h5>Check for Null Values </h5> 
```{r amelia-null-values, warning=FALSE}
# use missmap function from the Amelia package to check for NA values
missmap(vehicle_loan_default_train_cleaned,
        plot.background = element_rect(fill = "antiquewhite"),
        main = "Vehicle Loan Default - Missing Values", 
        x.cex = 0.45,
        y.cex = 0.6,
        margins = c(7.1, 7.1),
        col = c("yellow", "black"), legend = FALSE)
```
<br>
From the plot of missing values, we can see that there is no missing value in the data. No NA imputation or handling of nulls is necessary.  

<h5>Check for Imbalance of data for the <b>loan_default</b> variable </h5>  
```{r}
loan_default_grouped <- vehicle_loan_default_train_cleaned %>% group_by(loan_default) %>% 
                        summarise(count = n()) %>%
                        mutate(percentage = round((count / sum(count) * 100), 2))

loan_default_grouped_displayed <- kable(head(loan_default_grouped, 200), "html") %>% 
                                  kable_paper("hover", full_width = F)
loan_default_grouped_displayed
```


```{r bar-chart-loan-default-category}
p_bar_loan_default_category <- loan_default_grouped %>% ggplot(aes(x=factor(loan_default), 
                                                                   y = percentage, fill = factor(loan_default))) +
                               geom_bar(stat = "identity", position = "dodge") +
                               labs(title = "Loan Default Distribution", x = "Loan Default", y = "Percentage", 
                                    fill = "Loan Default") +
                               scale_y_continuous(labels = scales::percent_format(scale = 1)) +  # Format y-axis as percentages
                               theme_minimal()  + theme(plot.title = element_text(hjust = 0.5),
                                                        panel.background = element_rect(fill = "gray80"),
                                                        plot.background = element_rect(fill = "antiquewhite"))                          
p_bar_loan_default_category
```
<br>
From the values and plot above for the Loan Default categories, we can see that there are more values for category 0 (No default) than 1 (Default). Before, modeling the data, the data will be enhanced by oversampling the "default" category to achieve some form of balance in the data.  

<h5>Age Distribution</h5>  
```{r age-dist, warning=FALSE}
# Age distribution - All categories
age_dist_all <- vehicle_loan_default_train_cleaned %>% ggplot(aes(x = age_years)) +
                    geom_histogram(binwidth = 2, fill = "blue", color = "white", alpha = 0.7) +
                    labs(title = "Age Distribution - All Categories", x = "Age", y = "Frequency") + theme_minimal() +
                    theme(plot.title = element_text(hjust = 0.5),
                                                    panel.background = element_rect(fill = "gray80"),
                                                    plot.background = element_rect(fill = "antiquewhite"))

# Age distribution - Loan Default
age_dist_loan_default <- vehicle_loan_default_train_cleaned %>% filter(loan_default == 1) %>% ggplot(aes(x = age_years)) +
                    geom_histogram(binwidth = 2, fill = "blue", color = "white", alpha = 0.7) +
                    labs(title = "Age Distribution - Loan Default", x = "Age", y = "Frequency") + theme_minimal() +
                    theme(plot.title = element_text(hjust = 0.5),
                                                    panel.background = element_rect(fill = "gray80"),
                                                    plot.background = element_rect(fill = "antiquewhite"))

# Age distribution - No Default
age_dist_no_default <- vehicle_loan_default_train_cleaned %>% filter(loan_default == 0) %>% ggplot(aes(x = age_years)) +
                    geom_histogram(binwidth = 2, fill = "blue", color = "white", alpha = 0.7) +
                    labs(title = "Age Distribution - Loan Default", x = "Age", y = "Frequency") + theme_minimal() +
                    theme(plot.title = element_text(hjust = 0.5),
                                                    panel.background = element_rect(fill = "gray80"),
                                                    plot.background = element_rect(fill = "antiquewhite"))

# Plot all three plots
plot_age_dist <- plot_grid(age_dist_loan_default, age_dist_no_default, age_dist_all, byrow = TRUE, nrow = 3) 
plot_age_dist
```
<br>
We can see that most of the age distribution is about the same for both loan_default and no-loan_default. Also, we can see that most of the applicants fall between 20 years old and 40 years old with a few applicants between 40 and 60 and almost none after 70 years old.

<h5>CNS Score Description</h5>  
The <b>perform_cns_score_description</b> basically categorizes the risk level of the applicant based on their credit score.  
```{r cns_score_description, warning=FALSE}
# All Categories
cns_score_desc_all <- vehicle_loan_default_train_cleaned %>% group_by(perform_cns_score_description) %>% 
                                summarise(count = n()) %>%
                                mutate(percentage = round((count / sum(count) * 100), 2)) %>%
                                ggplot(aes(x = perform_cns_score_description, y = percentage,
                                                                    fill = factor(perform_cns_score_description))) +
                                geom_bar(stat = "identity", position = "dodge") +
                                labs(title = "Risk Level Distribution - All Categories", x = "Risk Level", y = "Percentage", 
                                     fill = "Risk Level") +
                                scale_y_continuous(labels = scales::percent_format(scale = 1)) +  # Format y-axis as percentages
                                theme_minimal()  + theme(plot.title = element_text(hjust = 0.5),
                                                         panel.background = element_rect(fill = "gray80"),
                                                         plot.background = element_rect(fill = "antiquewhite"))

cns_score_desc_all
```
<br>
Significantly high proportion of the applicants fall under low_risk category and over 85% fall under either very_low_risk or low_risk. This may be due to the believe that applicants may prefer to work on their credit to improve their risk level before apply for vehicle loans especially as low_risk applicants tend to get better interest rates on their loans.  

<h5> Loan Default Distribution By Risk Level</h5>
```{r loan-default-risk-level, warning=FALSE}
loan_default_risk_level <- vehicle_loan_default_train_cleaned %>% 
                               count(loan_default, perform_cns_score_description, name = "Record_Count") %>%
                               ggplot(aes(x=loan_default, y = Record_Count, fill = perform_cns_score_description)) +
                               geom_bar(stat = "identity", position = "stack") +
                               labs(title = "Loan Default Distribution by Risk Level", x = "Loan Default", y = "Percentage", 
                                    fill = "Risk Level") + theme_minimal()  + 
                               theme(plot.title = element_text(hjust = 0.5),
                                                        panel.background = element_rect(fill = "gray80"),
                                                        plot.background = element_rect(fill = "antiquewhite")) 
loan_default_risk_level
```
<br>
It is clear that a huge portion of the non-loan_default are categorized as low_risk or very_low_risk. Also, for the defaulters, the low_risk still constitute a huge portion.  

<h5>Employment Type</h5>  
```{r employment-type, warning=FALSE}
employment_type <- vehicle_loan_default_train_cleaned %>% 
                               count(loan_default, employment_type, name = "Record_Count") %>%
                               ggplot(aes(x=loan_default, y = Record_Count, fill = employment_type)) +
                               geom_bar(stat = "identity", position = "stack") +
                               labs(title = "Loan Default Distribution by Employment Type", x = "Loan Default", y = "Percentage", 
                                    fill = "Employment Type") + theme_minimal()  + 
                               theme(plot.title = element_text(hjust = 0.5),
                                                        panel.background = element_rect(fill = "gray80"),
                                                        plot.background = element_rect(fill = "antiquewhite")) 
employment_type
```
<br>
Salaried or Self-Employed constitute most of the data in almost equal proportions for both categories of default although the self-employed are a little more.  

<h5>Correlation Plot</h5>
```{r cor-plot, warning=FALSE, message=FALSE}
vehicle_loan_default_train_cleaned_numeric <- vehicle_loan_default_train_cleaned %>% 
                                              select(-employment_type, -perform_cns_score_description)
corr_matrix <- cor(vehicle_loan_default_train_cleaned_numeric)
correlation_plot <- ggcorrplot(corr_matrix, 
                               lab = TRUE, # Show axis labels
                               lab_size = 2, # Adjust the size of axis labels
                               hc.order = TRUE, # Reorder the correlation matrix
                               type = "lower", 
                               outline.col = "white", 
                               colors = c("blue", "white", "red"), 
                               ggtheme = ggplot2::theme_minimal(),
                               title = "Correlation Plot") + coord_fixed(ratio = 0.9) + 
                               theme(axis.text.x = element_text(size = 9, angle = 45, hjust = 1),
                                     axis.text.y = element_text(size = 9, hjust = 1),
                                     plot.title = element_text(hjust = 0.5),
                                     panel.background = element_rect(fill = "gray80"),
                                     plot.background = element_rect(fill = "antiquewhite"),
                                     axis.title = element_text(size = 10)) +labs(x = NULL, y = NULL)
```

```{r cor-plot-display, warning=FALSE}
correlation_plot
```

<br>

<h4> 5.2.2 Data Pre-processing, Model Training, and Testing </h4>
<h5>Preprocess the data</h5> 
Since the data is imbalanced, we preprocess the data to get a balanced dataset and also standardize the numeric variables in the data using the standard normal distribution.

```{r pre-processing}
data_preprocess_scaling <- function(df){
                      # This helper function standardizes the numeric variables of the df using the standard normal method
                      df <- as.data.frame(df)
                      df_char <- df %>% select(loan_default, employment_type, perform_cns_score_description) 
                      df_numeric <- df %>% select(-loan_default, -employment_type, -perform_cns_score_description)
                      df_numeric_scaled <- df_numeric %>% mutate_all( ~ (scale(.) %>% as.vector))
                      df_scaled_combined <- cbind(df_char, df_numeric_scaled)
                      return(df_scaled_combined)
}


xgb_nnet_preprocess <- function(df, mode){
 # convert the categorical variable to dummies
              df2 <- dummy_cols(df, select_columns = c("employment_type","perform_cns_score_description"), 
                                remove_selected_columns = TRUE) %>% as.data.frame()
              # prepare the xgb.DMatrix to use in the xgboost training
              df2_train <- as.data.frame(df2[, -1])
              df2_label <- as.data.frame(df2[, 1])
              df_dmatrix <- xgb.DMatrix(as.matrix(sapply(df2_train, as.numeric)), label=as.matrix(df2_label))
              
              if(mode == "xgboost"){
                return(df_dmatrix)
                
              } else if(mode == "nnet"){
                return(df2)
                
              } else{
                print("Mode not supported.")
              }
              
}


data_preprocessing <- function(df, mode = "train"){
    # This function pre-processes the cleaned data and get the data ready for training.
    tryCatch({
      if(is_tibble(df) | is.data.frame(df)){
        if(mode == "test"){
           df_scaled <- data_preprocess_scaling(df)
           print("Data Pre-processing complete")
           return(df_scaled)
           
        } else if(mode == "train"){
          curr_frame <<- sys.nframe() # sends the current frame to the global environment.
          # The ovun.sample function in the ROSE package assumes the data to be in the global env so you have to tell it  
          # which frame (scope) to find the data else this will fail if executed inside a function.
          df_ovun <- ovun.sample(formula = formula(loan_default ~ .), data = get("df", sys.frame(curr_frame)),
                                 N = 1.5 * nrow(data), seed = 1994, method = "both")$data %>% as.data.frame() %>% as_tibble()
          print("Oversampling and undersampling completed")
          df_scaled <- data_preprocess_scaling(df_ovun)
          print("Data Pre-processing complete!")
          return(df_scaled)
          
        } else {
          print("You did not enter a valid mode type: Enter train or test for mode")
          
        }
      }
      
    else{
    print("The dataframe is not a tibble. Kindly have your data in the form of a dataframe or a tibble")
  }
    
  },
  #if an error occurs, tell me the error
  error=function(e) {
        message('An Error Occurred')
        print(e)
        },
  #or if a warning occurs, tell me the warning
  warning=function(w) {
        message('A Warning Occurred')
        print(w)
        return(NA)
        }
    )
  
}
```

<h5>Train Test Split </h5> 
Use the CaTools library to split the cleaned dataset into training and testing datasets in 70:30 ratio.
```{r train-test-split}
# Set a seed
set.seed(1994)
#Split the sample
sampling <- sample.split(vehicle_loan_default_train_cleaned$loan_default, SplitRatio = 0.7) 
# Training Data
df_train_subset <- subset(vehicle_loan_default_train_cleaned, sampling == TRUE)
# Testing Data
df_test_subset <- subset(vehicle_loan_default_train_cleaned, sampling == FALSE)
```


Pre-process the train dataset
```{r preprocess-data}
df_train = data_preprocessing(df_train_subset, mode = "train")
```

Pre-process the test dataset
```{r}
df_test = data_preprocessing(df_test_subset, mode = "test")
```

<h5>Develop function to train the data </h5> 
The function `model_training` trains a machine learning model according to the mode selected (logistic, rf, xgboost, or nnet).

```{r define-model-training-function}
model_training <- function(df, mode = "logistic"){
  
          if(mode == "logistic"){
              print("Training a Logistic Regression Model...")
              logistic_model <- glm(formula = loan_default ~ . , 
                                       family = binomial(link = 'logit'), data = df)
              print("Logistic Regression Model complete")
              return(logistic_model)
           
        } else if(mode == "rf"){
              print("Training a Random Forest Classification Model...")
              rf_model_ranger <- ranger(
                                 formula   = loan_default ~ ., 
                                 data      = df, 
                                 num.trees = 500,
                                 mtry      = floor(length(df) / 3),
                                 probability = TRUE,
                                 verbose = FALSE,
                                 classification = TRUE
                                 )
              print("Random Forest Classification Model complete")
              return(rf_model_ranger)         
          
        } else if(mode == "xgboost"){
              print("Training an XGBoost Classification Model...")
              # pre-process the data to obtain the dmatrix
              df_dmatrix <- xgb_nnet_preprocess(df, mode)
              xgb_model <- xgboost(data = df_dmatrix, nthread = 4, nrounds = 150,
                                   max.depth = 10, eta = 0.1, objective = "binary:logistic", verbose = FALSE)
              print("XGBoost Training complete")
              return(xgb_model)
          
        } else if(mode == "nnet"){
              print("Training a Neural Network Classification Model...")
              # pre-process the model to convert character variables to dummies
              df_nnet <- xgb_nnet_preprocess(df, mode)
              nnet_model <- nnet(loan_default ~ ., data = df_nnet, decay = 5e-4, 
                                 size = 20, maxit = 100, trace = F, set.seed(1994))
              print("NNET Training complete")
              return(nnet_model)
          
        } else {
          print("You did not enter a valid mode type: Enter logistic, rf, xgboost or svm")
          
        }
      
}
```

<h5>Develop function to predict data </h5> 
The function `model_predictions` predicts the loan_default probability of new data for each of the model types.  

```{r define-model-predictions-function}
model_prediction = function(df, trained_model, model_type){
  
  # remove the response variable from the dataframe if it exists
  if("loan_default" %in% colnames(df)){
    test_data = df %>% select(-loan_default)
  } else {
    test_data = df
  }
  
  # make predictions
  if(model_type == "rf"){
    predictions = predict(trained_model, data = test_data)$predictions[,1]
  } else if (model_type == "logistic"){
    predictions = predict(trained_model, newdata = test_data, type = "response")
  }  else if (model_type == "xgboost"){
    test_data_xgb = xgb_nnet_preprocess(df, model_type)
    predictions = predict(trained_model, newdata = test_data_xgb)
  } else{
    test_data_nnet = xgb_nnet_preprocess(test_data, model_type)
    predictions = predict(trained_model, newdata = test_data_nnet)
  }
  
  # convert probabilities to classes
  predicted = ifelse(predictions > 0.5, 1, 0)
  
  return(predicted)
}
```

<h5>Develop function to evaluate metrics of the model </h5> 
The function `model_evaluations` provides metrics such as accuracy, recall, precision, f1-score, and AUC for the model.

```{r define-model-metrics-function}
model_metrics = function(actual, predicted){
  
  # accuracy
  accuracy_model = round((Metrics::accuracy(actual, predicted)), 4)
  # precision
  precision_model = round((Metrics::precision(actual, predicted)), 4)
  # recall 
  recall_model = round((Metrics::recall(actual, predicted)), 4)
  # auc
  auc_model = round((pROC::auc(actual, predicted)), 4)
  
  # model metrics
  model_eval_metrics = c(accuracy_model, precision_model, recall_model, auc_model) %>% t()
  column_names = c("Accuracy", "Precision", "Recall", "AUC")
  evaluation_metrics = data.frame(values = model_eval_metrics)
  colnames(evaluation_metrics) = column_names
  
  # confusion Matrix
  confusion_table = table(predicted, actual)
  confusion_matrix = caret::confusionMatrix(confusion_table)
  print("**********************************************************************")
  print(confusion_matrix)
  print("**********************************************************************")
  
  return(evaluation_metrics)
}
```



<h5> 5.2.2.1 Model Training  and Testing - Logistic Regression </h5>  

**Train the logistic model**  
```{r train-logistic-model}
logistic_model <- model_training(df_train, mode = "logistic")
```

**Evaluate the logistic Model**  
```{r evaluate-logistic-model, warning=FALSE, message=FALSE}
logistic_actual = df_test$loan_default
logistic_predicted = model_prediction(df_test, logistic_model, "logistic")
logistic_model_metrics = model_metrics(logistic_actual, logistic_predicted)
```

**Display Model Metrics - Logistic Regression Model**  
```{r display-logistic-metrics}
rownames(logistic_model_metrics) = "Logistic Model"
logistic_model_metrics 
```



<h5> 5.2.2.2 Model Training and Testing - Random Forest </h5>  

**Train a random forest classification model for the data**
```{r train-random-forest-model}
rf_model <- model_training(df_train, mode = "rf")
```

**Evaluate the Random Forest Model**  
```{r evaluate-rf-model, warning=FALSE, message=FALSE}
rf_actual = df_test$loan_default
rf_predicted = model_prediction(df_test, rf_model, "rf")
rf_model_metrics = model_metrics(rf_actual, rf_predicted)
```

**Display Model Metrics - Random Forest Classification Model** 
```{r display-rf-metrics}
rownames(rf_model_metrics) = "Random Forest Model"
rf_model_metrics 
```


<h5> 5.2.2.3 Model Training and Testing - XGBoost </h5>  
**Train an XGBoost classification model for the data**
```{r train-xgb-model, warning=FALSE, message=FALSE}
xgb_model <- model_training(df_train, mode = "xgboost")
```

**Evaluate the XGBoost Model**  
```{r evaluate-xgb-model, warning=FALSE, message=FALSE}
xgb_actual = df_test$loan_default
xgb_predicted = model_prediction(df_test, xgb_model, "xgboost")
xgb_model_metrics = model_metrics(xgb_actual, xgb_predicted)
```
**Display Model Metrics - XGBoost classification Model** 
```{r display-xgb-metrics}
rownames(xgb_model_metrics) = "XGBoost"
xgb_model_metrics
```

<h5> 5.2.2.4 Model Training and Testing - Artificial Neural Networks </h5>
**Train a Neural Network model for the data**
```{r train-nnet-model}
nnet_model <- model_training(df_train, mode = "nnet")
```

**Evaluate the NNET Model**  
```{r evaluate-nnet-model, warning=FALSE, message=FALSE}
nnet_actual = df_test$loan_default
nnet_predicted = model_prediction(df_test, nnet_model, "nnet")
nnet_model_metrics = model_metrics(nnet_actual, nnet_predicted)
```
**Display Model Metrics - Neural Network Model** 
```{r display-nnet-metrics}
rownames(nnet_model_metrics) = "Neural Network"
nnet_model_metrics
```

<h3> 5.3 Results </h3>
The table below compares the results of the four (4) models trained:  
```{r combine-results, fig.align='center'}
results = rbind(logistic_model_metrics, rf_model_metrics, xgb_model_metrics, nnet_model_metrics)
kable(results, "html") %>%
                        kable_paper("hover", full_width = F) %>%
                        scroll_box(width = "500px", height = "200px")
```
  

As we can see from the results of the different models. The models perform differently across the different metrics (Accuracy, Precision, Recall, AUC). It is important to know that the metric to be used to select the chosen model is dependent on the goal of the analysis/model.  
In this case of vehicle loan default prediction, the main goal is to identify applicants that will default on their loans. Also, kindly note that the positive class in this analysis is the zero (0 - No default) while the negative class is the 1 (Loan Default).  
Since the main concern is to correctly identifying defaults to minimize financial risk, the `Recall` is considered to be more critical. High recall ensures capturing most defaults, even at the cost of some false alarms.   
Recall measures the proportion of actual defaults that are correctly predicted by the model. It focuses on the model's ability to capture all positive instances of defaults. High recall means a low false negative rate, which is crucial when identifying all actual defaults is important, even if it means higher false positives.  
On the other hand, the precision measures the proportion of correctly predicted defaults out of all instances predicted as defaults. It specifically focuses on the accuracy of positive predictions, but here the focus is mainly on negative predictions which are default to minimize financial losses on vehicle loans. High precision means a low false positive rate, which is valuable when correctly identifying defaults is crucial and minimizing false alarms is a priority.  
Also, Accuracy measures the overall correctness of predictions by considering the ratio of correctly predicted instances (both defaults and non-defaults) to the total instances. While accuracy is an intuitive metric, its reliability can be affected by class imbalance which is usually the case for most loan-default problems as there are usually fewer defaults than non-defaults.  
Lastly, AUC quantifies the model's ability to distinguish between classes. It represents the area under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. It is a valuable metric for assessing the overall performance of a classification model, including its ability to discriminate between default and non-default instances in a loan default prediction problem. It provides a comprehensive evaluation of the model's discriminatory power, especially in scenarios with class imbalance.  

Looking at the results of the model, we find that that all of the models are competitive in terms of recall and they all performed badly on the precision and about average for the accuracy for the most part. Also, the AUC values are above average for the most part and I strongly believe that the overall metrics of these models can be improved by having more quality data, feature selection, and hyper-parameter tuning.  


<h3> 5.4 Limitations of the model and data </h3>  
One major limitation of the model is the data source. The data was obtained from Kaggle, which is an open database source, which may question the reliability of the data for real world usage. There are some features like credit score classification which may not be available for young applicants or immigrants to the country who have not built an history to be classified as low risk. More quality data is required to get a better performing model.  
Also, most of the models have tune-able parameters to obtain better performing models, but the models trained in this analysis did not include hyper-parameter tuning and at such, the best models might not have been obtained for each of the model categories. This tuning process was avoided here because some of the models like Neural Network, and the Tree-Based models can become quickly complex and may require more computational capacity to perform several cross-validation/grid search and tuning process. For example, for the Random Forest model, a mere 500 trees was randomly selected without knowing if that number of trees is enough to grow the Forest as well as other parameters. Also, the number of rounds for the Xtreme Gradient Boosting was randomly selected without finding the best values for the combination of tunable parameters to get the best model. Furthermore, for the Neural Network model, we do not know what depth of network or number of nodes to train the model on to get the best performing model.  
In addition, proper feature selection was not done to see if certain features can be dropped or to know what features of the data offer the most predictive value.  
Lastly, we did not compare training error to test error to know if the trained model suffers from either overfitting or underfitting.  

<h3> 5.5 Next Steps </h3>  
One of the most important next steps is to try to obtain better quality and reputable data possibly from a reputable financial institution. This might be very difficult to obtain considering the fact that financial data are highly regulated and also companies consider these data as part of their intellectual property and may not be willing to give these out. If these are done as part of an internal modeling process, better quality data may be obtained to be used in training a loan default model.  

In addition, it is very important to conduct hyper-parameter tuning for the models to determine what parameters would be best suited to train the model on. It is also important to know that as more data is included, the computational resources needed to conduct extensive hyper-parameter tuning on large data for complex models like Neural Network, and Tree-Based model would increase significantly especially for large datasets.  

Also, we can also try to do feature selection, check for multi-correlation using the Variance Inflation Factor (VIF) method/approach to obtain only relevant features that provide predictive value. Several other feature engineering techniques like principal component analysis (PCA) can also be explored.  

Furthermore, next steps should involve comparing training error vs test error to understand if the model is overfitting or underfitting and appropriate steps should be taken to minimize either underfitting or overfitting whichever is the case for the model.  
</br>

***

<center> <h2> 6. CONCLUSION </h2> </center>  
The purpose of this project was to develop a machine learning model for loan default prediction used in the automotive credit industry to minimize financial losses to financial institutions due to loan default. That is to say that banks are very much interested in identifying customers that are more likely to default on their auto loans and avoid extending credit to those customers. From the various literature reviews, we see that the industry standard for credit risk modeling/loan default prediction is currently the Logistic Regression. Throughout this study, machine learning algorithms such as Logistic regression, Random Forest, Extreme Gradient Boosting Machines and Neural Network have been examined and the findings is that the non-conventional tree-based models and the Neural Network models perform slightly better than the industry standard. The Industry standard of logistic regression is good enough and perform well compared to the other models but not necessarily better that others. However, since the Random Forest, XGBoost, and Neural Networks are very difficult to interpret, it may be difficult for banks to understand why a customer is rejected for their loan application. While on the other hand, the Logistic Regression model is relatively more easier to interpret than its counterparts, it might be difficult to sway loan officers/decision makers away from the logistic regression without extracting way better performance from the other models since the Logistic Regression performance is not too far from the others.  
Hence, I will still recommend the industry standard Logistic Regression model because of its simplicity, easier to understand and interpret, take less time to train and predict the results, and also provides competitive performance when compared to the other more complex models. Even though I recommend the Logistic Regression, I do not shut the doors on other models because as we continue to obtain more information about the behavior of customers in this increasingly digital world, the dimension of each observation might continue to increase and the Logistic Regression might suffer from getting predictive ability on highly dimensional data. Hence, the other models like Neural Network and Tree-based models might come in handy.  
</br>  

***
<center> <h2> 7. REFERENCES </h2> </center>  
Agarwal, S., Ambrose, B. W., & Chomsisengphet, S. (2008). Determinants of automobile loan default and prepayment. Economic Perspectives - Federal Reserve Bank of Chicago.  

Altman, E. I., & Saunders, A. (1998). Credit risk measurement: Developments over the last
20 years. Journal of Banking and Finance, 21(11-12), 1728–1742.

Agrawal, A., Agrawal, M., & Raizada, D. A. (2014). Predicting defaults in commercial vehicle
loans using logistic regression: Case of an indian nbfc. International Journal of Research in Commerce and Management, 5, 22–28.  

Brownlee, J. (2019, August 12). Overfitting and Underfitting With Machine Learning Algorithms. Machine Learning Mastery. Retrieved November 4, 2023, from https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/#:%7E:text=Overfitting%3A%20Good%20performance%20on%20the,poor%20generalization%20to%20other%20data

Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785-794).  

Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine learning, 20(3), 273-297.  

Crook, J. N., Edelman, D. B., & Thomas, L. C. (2007). Recent developments in consumer credit risk assessment. European Journal of Operational Research, 183, 447–1465.  

Cromer, O. C., Purdy, K. W., Cromer, G. C., & Foster, C. G. (2023, October 13). Automobile | Definition, History, industry, design, & Facts. Encyclopedia Britannica. https://www.britannica.com/technology/automobile  

Diez, D., Barr, C. D., & Cetinkaya-Rundel, M. (2019). OpenIntro statistics 4th Edition.  

Education, I. C. (2021, March 25). Underfitting. IBM Cloud Learn. Retrieved November 5, 2023, from https://www.ibm.com/cloud/learn/underfitting  

Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232.  

GeeksforGeeks. (2021, October 20). ML | Underfitting and Overfitting. Retrieved November 4, 2023, from https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/  

Hand, D.J. and Henley, W.E. (1997) Statistical Classification Methods in Consumer Credit Scoring: A Review. Journal of Royal Statistical Society, 160, 523-541.  

Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.  

Hao, C., Alam, M. M., & Carling, K. (2010). Review of the literature on credit risk modeling: Development of the past 10 years. Banks and Bank Systems, 5 (3).

Lessmann, S., Baesens, B., Seow, H.-V., & Thomas, L. C. (2015). Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research. European Journal of Operational Research, 247 (1), 124–136.  

Martin, A. (2023). What is an auto loan? Bankrate. https://www.bankrate.com/loans/auto-loans/what-is-an-auto-loan/  

Model Fit: Underfitting vs. Overfitting - Amazon Machine Learning. (n.d.). Amazon Machine Learning Developer Guide. Retrieved November 4, 2023, from https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html

O’Brien, S. (2023, February 4). Auto loan delinquencies are rising. Here’s what to do if you’re struggling with payments. CNBC. https://www.cnbc.com/2023/02/04/auto-loan-delinquencies-rise-what-to-do-if-you-struggle-with-payments.html  

Probasco, J. (2023). Expert explanation of how auto loans work. Investopedia. https://www.investopedia.com/how-car-loans-work-5202265  

U.S. vehicle fleet 1990-2021 | Statista. (2023, August 24). Statista. https://www.statista.com/statistics/183505/number-of-vehicles-in-the-united-states-since-1990/  

U.S.: average selling price of new vehicles 2022 | Statista. (2023, June 7). Statista. https://www.statista.com/statistics/274927/new-vehicle-average-selling-price-in-the-united-states/  

Witkowski, R. (2023, June 2). For People Under 30, Car Loan Delinquencies Hit A 15-Year High. Is The Economy Running Out Of Gas? Forbes Advisor. https://www.forbes.com/advisor/auto-loans/car-loan-late-payments/  
</br>

***
<center> <h2> 8. APPENDIX </h2> </center> 
This section contains all codes used in the analysis in the correct order. However, no output is generated.  
```{r appendix-all-codes, warning=FALSE, message=FALSE, eval=FALSE}
# Load libraries
library(Amelia) # To visualize missing data
library(caret)
library(caTools)
library(corrplot) # To plot correlation plot
library(cowplot) # To combine plots in a grid
library(fastDummies) # to convert character variables to dummies
library(ggcorrplot) # To plot correlation plot
library(kableExtra)
library(Metrics) # for model evaluation
library(nnet)
library(pROC)
library(ranger) # for random forest implementation
library(ROSE) # to balance the data
library(tidyverse)
library(xgboost)

# Read the data
url_vehicle_loan_default_train = "https://raw.githubusercontent.com/chinedu2301/data698-analytics-project/main/data/vehicle_default_train_data.csv"
url_vehicle_loan_default_test = "https://raw.githubusercontent.com/chinedu2301/data698-analytics-project/main/data/vehicle_default_test_data.csv"
vehicle_loan_default_train_raw = read_csv(url_vehicle_loan_default_train) %>% as_tibble()
vehicle_loan_default_test_raw = read_csv(url_vehicle_loan_default_test) %>% as_tibble()

# data_cleaning
data_cleaning <- function(df){
  # This function accepts a dataframe (df) as input and returns another dataframe (cleaned_df) that is clean.
  tryCatch({
  if(is_tibble(df) | is.data.frame(df)){
    print("The dataframe is a tibble, will proceed to clean data")
    print("Data Cleaning in progress...")
    # rename all the columns in the dataframe to lowercase
    cleaned_df <- df %>% rename_all(tolower) %>%  
                # compute the age of the applicant in number of years
                mutate(date_of_birth = dmy(date_of_birth),  
                      disbursal_date = dmy(disbursal_date), 
                      age = difftime(disbursal_date, date_of_birth, units = "days"),
                      age_years = round(as.numeric(age / 365.25), 0)) %>%
               # extract the years and month component of the average_acct_age  and convert to years
                mutate(average_acct_age_year_comp = as.numeric(str_extract(average_acct_age, "\\d+")),
                      average_acct_age_mon_comp = as.numeric(str_extract(average_acct_age, "\\d+(?=mon)")),
                      average_acct_age = round((average_acct_age_year_comp + average_acct_age_mon_comp/12), 0)
                       ) %>%
                # extract the years and month component of the credit_history_length and convert to years
                mutate(credit_history_length_year_comp = as.numeric(str_extract(credit_history_length, "\\d+")),
                       credit_history_length_comp = as.numeric(str_extract(credit_history_length, "\\d+(?=mon)")),
                       credit_history_length = round((credit_history_length_year_comp + credit_history_length_comp/12), 0)
                       )  %>%
                # clean up the perform_cns_score_distribution to include only a few categories
                mutate(lowercase_cns_description = tolower(perform_cns_score_description),
                       perform_cns_score_description = case_when(
                                str_detect(lowercase_cns_description, "very low risk") ~ "very_low_risk",
                                str_detect(lowercase_cns_description, "low risk") ~ "low_risk",
                                str_detect(lowercase_cns_description, "medium risk") ~ "medium_risk",
                                str_detect(lowercase_cns_description, "high risk") ~ "high_risk",
                                str_detect(lowercase_cns_description, "very high risk") ~ "very_high_risk",
                                str_detect(lowercase_cns_description, "not scored|no bureau") ~ "low_risk",
                                TRUE ~ "none"))  %>%
                # clean up the employment type to have only few categories
                mutate(lower_case_employment_type = tolower(employment_type),
                       employment_type = case_when(
                                str_detect(lower_case_employment_type, "salaried") ~ "salaried",
                                str_detect(lower_case_employment_type, "self employed") ~ "self_employed",
                                TRUE ~ "not_reported")) %>%
                # select only the required columns
                select(
                  age_years, disbursed_amount, asset_cost, ltv, employment_type, perform_cns_score_description,
                  pri_no_of_accts, pri_active_accts, pri_overdue_accts, pri_current_balance, pri_sanctioned_amount,
                  pri_disbursed_amount, sec_no_of_accts, sec_active_accts, sec_overdue_accts, sec_current_balance,
                  sec_sanctioned_amount, sec_disbursed_amount, primary_instal_amt, sec_instal_amt, new_accts_in_last_six_months,
                  delinquent_accts_in_last_six_months, average_acct_age, credit_history_length, no_of_inquiries, loan_default
                )
                print("Data Cleaning complete!!!")
    return(cleaned_df)
    
  }
  else{
  print("The dataframe is not a tibble. Kindly have your data in the form of a dataframe or a tibble")
  }
    
  },
  #if an error occurs, tell me the error
  error=function(e) {
        message('An Error Occurred')
        print(e)
        },
  #or if a warning occurs, tell me the warning
  warning=function(w) {
        message('A Warning Occurred')
        print(w)
        return(NA)
        }
    )
  
}

# clean the data
vehicle_loan_default_train_cleaned = data_cleaning(vehicle_loan_default_train_raw)

# data pre-processing
data_preprocess_scaling <- function(df){
                      # This helper function standardizes the numeric variables of the df using the standard normal method
                      df <- as.data.frame(df)
                      df_char <- df %>% select(loan_default, employment_type, perform_cns_score_description) 
                      df_numeric <- df %>% select(-loan_default, -employment_type, -perform_cns_score_description)
                      df_numeric_scaled <- df_numeric %>% mutate_all( ~ (scale(.) %>% as.vector))
                      df_scaled_combined <- cbind(df_char, df_numeric_scaled)
                      return(df_scaled_combined)
}


xgb_nnet_preprocess <- function(df, mode){
 # convert the categorical variable to dummies
              df2 <- dummy_cols(df, select_columns = c("employment_type","perform_cns_score_description"), 
                                remove_selected_columns = TRUE) %>% as.data.frame()
              # prepare the xgb.DMatrix to use in the xgboost training
              df2_train <- as.data.frame(df2[, -1])
              df2_label <- as.data.frame(df2[, 1])
              df_dmatrix <- xgb.DMatrix(as.matrix(sapply(df2_train, as.numeric)), label=as.matrix(df2_label))
              
              if(mode == "xgboost"){
                return(df_dmatrix)
                
              } else if(mode == "nnet"){
                return(df2)
                
              } else{
                print("Mode not supported.")
              }
              
}


data_preprocessing <- function(df, mode = "train"){
    # This function pre-processes the cleaned data and get the data ready for training.
    tryCatch({
      if(is_tibble(df) | is.data.frame(df)){
        if(mode == "test"){
           df_scaled <- data_preprocess_scaling(df)
           print("Data Pre-processing complete")
           return(df_scaled)
           
        } else if(mode == "train"){
          curr_frame <<- sys.nframe() # sends the current frame to the global environment.
          # The ovun.sample function in the ROSE package assumes the data to be in the global env so you have to tell it  
          # which frame (scope) to find the data else this will fail if executed inside a function.
          df_ovun <- ovun.sample(formula = formula(loan_default ~ .), data = get("df", sys.frame(curr_frame)),
                                 N = 1.5 * nrow(data), seed = 1994, method = "both")$data %>% as.data.frame() %>% as_tibble()
          print("Oversampling and undersampling completed")
          df_scaled <- data_preprocess_scaling(df_ovun)
          print("Data Pre-processing complete!")
          return(df_scaled)
          
        } else {
          print("You did not enter a valid mode type: Enter train or test for mode")
          
        }
      }
      
    else{
    print("The dataframe is not a tibble. Kindly have your data in the form of a dataframe or a tibble")
  }
    
  },
  #if an error occurs, tell me the error
  error=function(e) {
        message('An Error Occurred')
        print(e)
        },
  #or if a warning occurs, tell me the warning
  warning=function(w) {
        message('A Warning Occurred')
        print(w)
        return(NA)
        }
    )
  
}

# Train Test Split
# Set a seed
set.seed(1994)
#Split the sample
sampling <- sample.split(vehicle_loan_default_train_cleaned$loan_default, SplitRatio = 0.7) 
# Training Data
df_train_subset <- subset(vehicle_loan_default_train_cleaned, sampling == TRUE)
# Testing Data
df_test_subset <- subset(vehicle_loan_default_train_cleaned, sampling == FALSE)


df_train = data_preprocessing(df_train_subset, mode = "train")
df_test = data_preprocessing(df_test_subset, mode = "test")

# Model training function
model_training <- function(df, mode = "logistic"){
  
          if(mode == "logistic"){
              print("Training a Logistic Regression Model...")
              logistic_model <- glm(formula = loan_default ~ . , 
                                       family = binomial(link = 'logit'), data = df)
              print("Logistic Regression Model complete")
              return(logistic_model)
           
        } else if(mode == "rf"){
              print("Training a Random Forest Classification Model...")
              rf_model_ranger <- ranger(
                                 formula   = loan_default ~ ., 
                                 data      = df, 
                                 num.trees = 500,
                                 mtry      = floor(length(df) / 3),
                                 probability = TRUE,
                                 verbose = FALSE,
                                 classification = TRUE
                                 )
              print("Random Forest Classification Model complete")
              return(rf_model_ranger)         
          
        } else if(mode == "xgboost"){
              print("Training an XGBoost Classification Model...")
              # pre-process the data to obtain the dmatrix
              df_dmatrix <- xgb_nnet_preprocess(df, mode)
              xgb_model <- xgboost(data = df_dmatrix, nthread = 4, nrounds = 150,
                                   max.depth = 10, eta = 0.1, objective = "binary:logistic", verbose = FALSE)
              print("XGBoost Training complete")
              return(xgb_model)
          
        } else if(mode == "nnet"){
              print("Training a Neural Network Classification Model...")
              # pre-process the model to convert character variables to dummies
              df_nnet <- xgb_nnet_preprocess(df, mode)
              nnet_model <- nnet(loan_default ~ ., data = df_nnet, decay = 5e-4, 
                                 size = 20, maxit = 100, trace = F, set.seed(1994))
              print("NNET Training complete")
              return(nnet_model)
          
        } else {
          print("You did not enter a valid mode type: Enter logistic, rf, xgboost or svm")
          
        }
      
}

# model prediction function
model_prediction = function(df, trained_model, model_type){
  
  # remove the response variable from the dataframe if it exists
  if("loan_default" %in% colnames(df)){
    test_data = df %>% select(-loan_default)
  } else {
    test_data = df
  }
  
  # make predictions
  if(model_type == "rf"){
    predictions = predict(trained_model, data = test_data)$predictions[,1]
  } else if (model_type == "logistic"){
    predictions = predict(trained_model, newdata = test_data, type = "response")
  }  else if (model_type == "xgboost"){
    test_data_xgb = xgb_nnet_preprocess(df, model_type)
    predictions = predict(trained_model, newdata = test_data_xgb)
  } else{
    test_data_nnet = xgb_nnet_preprocess(test_data, model_type)
    predictions = predict(trained_model, newdata = test_data_nnet)
  }
  
  # convert probabilities to classes
  predicted = ifelse(predictions > 0.5, 1, 0)
  
  return(predicted)
}

# Model Metrics
model_metrics = function(actual, predicted){
  
  # accuracy
  accuracy_model = round((Metrics::accuracy(actual, predicted)), 4)
  # precision
  precision_model = round((Metrics::precision(actual, predicted)), 4)
  # recall 
  recall_model = round((Metrics::recall(actual, predicted)), 4)
  # auc
  auc_model = round((pROC::auc(actual, predicted)), 4)
  
  # model metrics
  model_eval_metrics = c(accuracy_model, precision_model, recall_model, auc_model) %>% t()
  column_names = c("Accuracy", "Precision", "Recall", "AUC")
  evaluation_metrics = data.frame(values = model_eval_metrics)
  colnames(evaluation_metrics) = column_names
  
  # confusion Matrix
  confusion_table = table(predicted, actual)
  confusion_matrix = caret::confusionMatrix(confusion_table)
  print("**********************************************************************")
  print(confusion_matrix)
  print("**********************************************************************")
  
  return(evaluation_metrics)
}

# Train Logistic Model
logistic_model <- model_training(df_train, mode = "logistic")
logistic_actual = df_test$loan_default
logistic_predicted = model_prediction(df_test, logistic_model, "logistic")
logistic_model_metrics = model_metrics(logistic_actual, logistic_predicted)
rownames(logistic_model_metrics) = "Logistic Model"

# Train RF model
rf_model <- model_training(df_train, mode = "rf")
rf_actual = df_test$loan_default
rf_predicted = model_prediction(df_test, rf_model, "rf")
rf_accuracy = accuracy(rf_actual, rf_predicted)
rf_model_metrics = model_metrics(rf_actual, rf_predicted)
rownames(rf_model_metrics) = "Random Forest Model"

# Train an XGBoost model
xgb_model <- model_training(df_train, mode = "xgboost")
xgb_actual = df_test$loan_default
xgb_predicted = model_prediction(df_test, xgb_model, "xgboost")
xgb_model_metrics = model_metrics(xgb_actual, xgb_predicted)
rownames(xgb_model_metrics) = "XGBoost"

# Train an NNET model
nnet_model <- model_training(df_train, mode = "nnet")
nnet_actual = df_test$loan_default
nnet_predicted = model_prediction(df_test, nnet_model, "nnet")
nnet_model_metrics = model_metrics(nnet_actual, nnet_predicted)
rownames(nnet_model_metrics) = "Neural Network"

# Result
results = rbind(logistic_model_metrics, rf_model_metrics, xgb_model_metrics, nnet_model_metrics)
kable(results, "html") %>%
                        kable_paper("hover", full_width = F) %>%
                        scroll_box(width = "500px", height = "200px")
```


