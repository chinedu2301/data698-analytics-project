---
title: "DATA698 Project - Vehicle Loan Default Prediction"
author: "Chinedu Onyeka"
date: "12/03/2023"
output:
  html_document:
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
    code_folding: "hide"
  pdf_document: default
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<center> <h2> 0. ABSTRACT </h2> </center>  
This project aims to predict the default of auto-loans using Machine Learning techniques. The dataset used was gotten from kaggle and contains about 41 variables and over 230,000 records of previous loan applicants.  
Most banks typically uses Logistic Regression Model to make decision whether an applicant is at risk of default or not. However, in this project, several other machine learning techniques were explored such as Tree-based models like Random Forest, and XGBoost (Extreme Gradient Boosting) as well as the Support Vector Machines (SVM) and the Logistic Regression Models were used. However, due to time and resources constraints, the models were not fully exploited to get the best out of each of the models. Simply, hyper-parameter tuning was not done on the models that have tunable parameters.

<center> <h2> 1. INTRODUCTION </h2> </center> 
<h3> 1.1 Research Problem </h3>  
The problem that this project aims to solve is to develop a machine learning model that will predict whether a potential borrower will default on their loan or not. 
The auto-loan industry is a multi-billion dollar industry that affects almost all aspects of life in the developed world. Automobiles are now the de facto means of movement in suburbs and even in some major cities and the volume of automobiles on the roads has increased tremendously over the past decades. Buying an automobile also referred to in this project as vehicle is an important decision for most people and banks are usually at the center of this decision since majority of vehicle purchases are made through financing option. Hence, financial institutions like banks have to face this particular problem all the time to decide whether a borrower will be able to make payments throughout the lifetime of the loan.  
Banks do not want to issue loans to individuals that would default and at the same time do not want to deny loans for individuals that would not default. To do this, they want to be able to predict to a high level of confidence whether to approve or deny an auto-loan so that they will be able to minimize losses and writeoffs when a borrower is not able to make payments and at the same time make profits from customers who are able to service their loans. This project aims to develop a model using machine learning techniques to help banks decide which potential borrower is likely to default on their loans or not.  
</br>
<h3> 1.2 Definition of Terms </h3>  
**Automobile: ** Automobile, byname auto, also called motorcar or car, a usually four-wheeled vehicle designed primarily for passenger transportation and commonly propelled by an internal-combustion engine using a volatile fuel (Cromer et al., 2023). Automobiles could also be referred to as a vehicle, motor vehicle, light trucks, etc. These days, automobiles can be propelled by either an internal-combustion engine or by electric battery. About 282 million vehicles were registered in the United States in 2021 about 90 million more compared to about 193 million in 1990. (U.S. Vehicle Fleet 1990-2021 | Statista, 2023). Also, the average price of motor vehicles in the United States in 2022 is about $46,000 USD  (U.S.: Average Selling Price of New Vehicles 2022 | Statista, 2023)  
**Auto Loan: ** An auto loan is the money you borrow to pay for your car. You will have to repay the loan and interest in fixed installments (Martin, 2023). Auto Loans are also referred to as car loan, vehicle loan, car financing, etc. These loans are often secured loans meaning that the car is used as the collateral to secure the loan. Typically, consumers borrow money to buy vehicles. In fact, consumers owed about 1.41 trillion US dollars on vehicles they drove in 2022. Also, the average auto loan balance is about $22,000 USD. In addition, about 80% of all new vehicles on the road is financed through a loan or lease (Chris, 2023). This goes to tell us that the auto-loan industry is a multi-trillion dollar industry.  
**Vehicle Loan Default: ** Loan default refers to when a borrower fails to make their installment payments as agreed by the loan terms. Usually for secured loans, the lender (bank in this case) can repossess the asset (car) which is used as collateral for the loan. Banks usually do not want to do that but sometimes are forced to do that if the borrower defaults and does not make an arrangement with the lender. When a lender repossess the car, the value of the car at the time of repossession may not be up to the loan balance and the lender will have no option than to write off that balance as a loss. In the US, default rates for auto loans are on the rise and currently sits at about 2%. The benefits of being able to predict whether a borrower will default cannot be over emphasized as it will not only help lenders to know whether to approve or deny a loan application, it will also help them price the interest rate for lenders appropriately.  
The problem of loan default is in all ramifications very probabilistic and it's often also considered strongly in credit risk modeling/credit scoring. Hence, lenders use a vast amount of credit risk tool to determine whether a borrower is likely to default or not. Entering default — when your lender determines you are not going to pay, usually some time after 90 days of no payments — can translate into your car being repossessed (O’Brien, 2023).
</br>

* * *
<center> <h2> 2. LITERATURE REVIEW </h2> </center>  
The probability of loan default is a well researched topic especially because the industry is a multi-trillion dollar industry. This area is of significant economic importance and often also regarded as credit risk modeling or credit scoring. We review literature related to credit in general and then those related to auto-loan.  

In Crook et. al.(2007), Credit scoring is concerned with developing empirical models to support decision making in the retail credit business. Also, a credit score is a model-based estimate of the probability that a borrower will show some undesirable behavior in the future. In application scoring, for example, lenders employ predictive models, called scorecards, to estimate how likely an applicant is to default. Such PD (probability of default) scorecards are routinely developed using classification algorithms (e.g., Hand & Henley, 1997).  

Whilst the extension of credit goes back to Babylonian times, the history of credit scoring began in 1941 with the publication by Durand of a study that distinguished between good and bad loans made by 37 firms (Crook et al. 2007). Since then the already established techniques of statistical discrimination have been developed and an enormous number of new classification algorithms have been researched and tested. Virtually all major banks use credit scoring with specialized consultancies providing credit scoring services and offering powerful software to score applicants, monitor their performance and manage their accounts.  

Altman and Saunders (1998) published an overview of credit risk modelling for the last 20 years. They found that credit risk modelling has evolved drastically for the past 20 years due to new emerging statistical techniques(Altman & Saunders, 1998). Later, another group of researchers published an extension to Altman and Saunders work presenting a further
development of credit risk modelling(Hao, Alam, & Carling, 2010). Their work identified more than 1000 articles on this topic, and found that logistic regression (LR) model and discriminant analysis are the most widely used methods for constructing scoring systems.

Also, Crook et al.(2007) conducted a research on credit risk scoring and found that the commonest method of estimating a classifier of applicants into those likely to repay and those unlikely to repay is logistic regression with the logit value compared with a cut off. Basically, this research makes claim that the industry standard for predicting loan default is the logistic regression model.  

Lessmann et al.(2015) compared about 41 classifiers based on six performance measures across eight real-world credit scoring data sets from the UK, Europe, and Australia. They investigated the overall model performance using several datasets, and examine the predicting performance in each case. The conclusion from this research suggests that several classifiers predict risk significantly better than the industry standard of Logistic Regression (LR). It went further to recommend the Random Forests(RF) model as a benchmark model because of its effectiveness, precision, and its interpretability.  

Agrawal et al.(2014) studied the impact of contract-specific variables as predictors in commercial vehicle loans. In their research, applying a logistic regression model for predicting default, around 11 out of 17 contract-specific variables where identified to provide additional assistance for the credit lending institution(Agrawal, Agrawal, & Raizada, 2014). The authors also suggest that contract information could improve the accuracy in more advanced nonlinear models. Specifically, the authors suggest the use of Neural Networks as one potential predictive model to improve the performance based on contract information(Agrawal et al.,2014).  

Keeping the outcome of the above literature in mind, this project aims to contribute to the field of vehicle loan prediction by developing machine learning models that will be able to predict vehicle loan default based on the available data, and  also compare  three (3) models: Random Forest, XGBoost, and SVM models to the industry standard Logistic Regression model. The data set used in this project contains more data than those used in most of the literature reviewed above. Statistically, more data provides better results and we hope that will be useful in better comparing the model and decide which model provides the best prediction metrics. In this work, we explain the data used, the pre-processing and feature selection involved and also provide a quick overview of predictive analytics as well as quick review to help understand the scoring metrics for classification problems. In addition, each of the models used are briefly explained and then the analysis of the data followed by modeling/testing and then conclusion on the findings.
</br>

* * *

<center> <h2> 3. OVERVIEW OF THE MODELS </h2> </center> 
<h3> 3.1 Predictive Analytics Overview </h3>  
Predictive analytics involves using historical data, statistical algorithms, and machine learning techniques to predict future outcomes. There are different types of algorithms for making predictions. Generally, we have regression models and classification models. Regression algorithms are mainly used when the variable to be predicted is a continuous value while classification algorithms are used to predict categories or classes.Classification algorithms are a fundamental part of predictive analytics and are used to categorize data into classes or groups based on specific features or attributes. Examples of classification algorithms include but not limited to Logistic Regression, Decision Trees, Random Forest, XGBoost, SVM, KNN, Naive Bayes, etc. The problem of vehicle loan default prediction is a classification problem and the classification algorithms that will be used to solve the problem of loan default in this project are briefly explained below:

<h3> 3.2 Logistic Regression </h3>  
Logistic Regression is a type of classification algorithm that is used when the response variable is binary in nature. Being binary means that the response variable is a two level categorical variable. The Logistic Regression model belongs to the family of generalized linear models (GLMs) and it's used when the response is a two-level categorical variable. Typical examples of binary response variables are Yes/No; Male/Female; Cancer/No Cancer; Approve/Deny, etc. These response variables are often coded as 1 or 0. Even though the name contains regression, the logistic regression model is often used when the response variable is discrete. i.e It is a classification algorithm.  
Logistic Regressions must not necessarily be binary as there are possible situations where there are more than two-level categories in the response variables and such situations are regarded as multinomial logistic regression which is beyond the scope of this article.  
The logistic regression model relates the probability that a response variable would be successful to the predictors $x_{1, i}, x_{2, i}, ..., x_{k, i}$ through a framework like that of multiple regression:  
$logit(p_{i}) = log_{e}(\frac{p_{i}}{1-p_{i}}) = \beta_{0} + \beta_{1}x_{1,i} + \beta_{2}x_{2,i} + ... + \beta_{k}x_{k,i}$

<center> <b>Assumptions for Logistic Regression</b> </center>  
<li> Each outcome of the response variable is independent of the other outcomes. </li> 
<li> The response variable must follow a binomial distribution. </li> 
<li> Each predictor $x_{i}$ is linearly related to the $logit(p_{i})$ if other predictors are held constant. </li>  

<h3> 3.3 Tree-Based Methods </h3>  
Tree-based models are a type of supervised learning algorithm used for both classification and regression tasks. They construct a decision tree that recursively splits the data into subsets based on the most significant features. The tree structure consists of nodes representing the features, edges representing the decision rules, and leaves representing the output (class or value). The key advantage of tree-based models is their ability to handle non-linear relationships and interactions between features. However, they are prone to overfitting, especially when the trees become too complex.  
<b>Bagging (Bootstrap Aggregating): </b> It's a technique that aims to reduce variance and prevent overfitting by training multiple models on different bootstrapped subsets of the dataset and then averaging the predictions. Random Forest is an ensemble method based on bagging, utilizing multiple decision trees trained on different subsets of the data.  
<b>Boosting: </b> Boosting is an ensemble technique that combines weak learners (typically shallow trees) sequentially to create a strong model. It focuses on improving the shortcomings of its predecessors by assigning higher weight to misclassified data, effectively learning from previous mistakes. Gradient Boosting and XGBoost are examples of boosting algorithms.  

<h4> 3.3.1 Random Forest </h4>  
It's an ensemble learning method that constructs multiple decision trees and merges their predictions to improve accuracy and reduce overfitting. It introduces randomness both in feature selection and dataset bootstrapping. By aggregating predictions from various trees, it tends to be more robust and less prone to overfitting compared to a single decision tree.  

<h4> 3.3.2 Gradient Boosting </h4>  
This is a boosting technique that builds trees sequentially. It fits each tree to the residuals (errors) of the preceding tree, reducing the errors at each step. Gradient boosting is a powerful algorithm known for its ability to handle complex data and achieve high accuracy.  

<h4> 3.3.2 XGBoost (Extreme Gradient Boosting) </h4>  
It is an optimized and highly efficient implementation of gradient boosting. XGBoost improves upon the traditional gradient boosting method by introducing regularization, parallel computing, and a variety of enhancements that significantly speed up the training process and improve accuracy.

<h3> 3.4 Support Vector Machines </h3>  
Support Vector Machines (SVM) are powerful supervised learning models used for both classification and regression tasks. The primary goal of SVM is to find the optimal hyperplane that best separates data points into different classes. SVM works by finding the hyperplane that maximizes the margin between different classes in a dataset. This hyperplane is the one that creates the largest gap between the closest data points of different classes, also known as support vectors. SVM can handle non-linear data by mapping the input data into a higher-dimensional space using a kernel function (e.g., polynomial, radial basis function), where a linear separation is then performed.  
The objective of SVM is to maximize the margin between classes while minimizing classification errors. It aims to generalize well on unseen data by finding the optimal decision boundary. In SVM, the regularization parameter (C) controls the trade-off between maximizing the margin and minimizing the classification error. A higher C value allows more points to be classified correctly but might lead to overfitting. They are originally designed for binary classification, but can be extended to handle multi-class problems using various strategies like one-vs-one or one-vs-rest.  
The downside of SVM is that training an SVM on a large dataset can be computationally expensive, especially when using non-linear kernels or dealing with a high number of features. Also, Selecting an appropriate kernel and tuning its parameters can be challenging. It requires domain knowledge and often involves some trial and error. In addition, SVM can be sensitive to noise in the data, which might result in overfitting, especially if the margin is overly optimized.


<h3> 3.5 Scoring the Model and Understanding the Scoring Metrics </h3>  
After obtaining and fitting a classification model, we need certain metrics to evaluate how well the model performs.Below are some metrics that can be used to evaluate the performance of a classification model:  
<center>
<img src="https://github.com/chinedu2301/data621-business-analytics-data-mining/blob/main/blogs/confusion-matrix.png?raw=true" />  
source: https://www.debadityachakravorty.com/ai-ml/cmatrix/
</center>  
<br>
The type of metrics to be used depends on the situation.
<li> **Accuracy: **  This is the most common measure used to evaluate a classification model. Accuracy is the ratio of correctly classified observations to the total. This tells us the percentage of observations that our model is correctly classifying.   
$accuracy = \frac{TP + TN}{TP+FP+TN+FN}$  
Accuracy is great for symmetric datasets and when the cost of false positives and false negatives are similar.</li>  
<li> **Precision: ** This is the percentage of the results that are relevant. It is computed by calculating the number of true positives (TP) divided by all positively classified observations by the model (both TP and FP). The precision tells us the percentage of observations that are actually positive from all positively classified observations by the model. i.e a precision of 90% means that of all the observations that are classified as positive by our model, only 90% of those are actually positive which means that the model correctly classified the positive cases in 90% of the cases.  
$precision = \frac{TP}{TP+FP}$  
We use precision when we want to be more confident about our true positives. For example, in spam/ham emails, you want to be sure that the email is spam before putting it in the spam box. </li> 
<li> **Recall (Sensitivity): ** The recall is also regarded as the sensitivity of the model. This refers to the percentage of total relevant results that are correctly classified by the model. Essentially, it means the ratio of the number of positively classified observations to the total number of actual positives both those correctly classified and incorrectly classified by the model (TP and FN). It is computed by dividing the number of true positives by the total number of observations that are actually positive (whether correctly classified by the model or not). The Recall also known as Sensitivity tells us what proportion of the positive class got correctly classified. i.e What percentage of cancer patients got correctly classified as cancer patients by the model. A recall of 95% means that of all the actually positive cases, only 95% were correctly classified by the model.  
$recall = \frac{TP}{TP+FN}$  
We use recall when having a false positive is way better than having a false negative. for example, you want to tell someone that they have cancer (FP) which in fact they don't instead of telling them that they don't have cancer(FN) when infact they have it. It would be disastrous to give a False Negative to a cancer patient because they would probably have had time to ameliorate the situation if they had been informed earlier. The recall is better when the cost of false negatives is unacceptable. i.e. False positive is better than false negative.</li> 
<li> **F1 Score: ** This is the harmonic mean of the precision and recall. It is also called F-score or F-Measure.  
$F1 = \frac{2 * precision * recall}{precision + recall}$  
F1 is best for uneven distribution and it can be used to compare different models.</li>  


<h3> 3.6 Underfitting and Overfitting </h3>  
In supervised machine learning problems, we aim to get a model that properly fits the training data and also generalizes well on new/test data. When the model is unable to generalize on new data, it is not able to perform its purpose. One of such causes could be as a result of underfitting or overffiting.  
<li> **Undefitting: ** This is a situation in machine learning where a model does not properly fit the training data resulting in high training error and high test error. In this case, the model performs poorly on the training data as well as on new data.  An underfit machine learning model is not a suitable model for the data because it is not able to properly capture the relationship between the input examples (X) and the target values (Y).Poor performance on the training data could mean that the model is too simple to describe the target properly. A typical example of an underfit model is using a linear model for data points with quadratic relationship.  
Since the model cannot generalize well on new data, it cannot be leveraged for prediction or classification tasks. High bias and low variance are good indicators of underfitting. </li>
<li> **Overfitting: ** This is simply the opposite of underfitting. It is a situation in machine learning where a model properly fits the training data, but performs poorly on new datasets thereby resulting in low training error, but high test error. It involves good performance on training data, but poor performance on new/test data. The model is essentially learning the noise and details in the training data and memorizing it such that it can not generalize to unseen data. A typical example of overfitting is fitting a quadratic function with a cubic or higher order polynomial model. High variance and low bias are indicators of overfitting. </li>  
<br>

<center>
<img src = "https://github.com/chinedu2301/data621-business-analytics-data-mining/blob/main/blogs/underfitting-overfitting-aws.png?raw=true" />
source: https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html
</center>
<br>

<center> <b> Reducing Underfitting  </b> </center>  
There are different ways to decrease underfitting such are:
<li>Increase model complexity. It could be that the selected model is too simple and a more complex model may be required.</li>
<li>Perform feature selection/feature engineering. </li>
<li>Increase the duration of training to get better results.</li>
<li>Decrease the amount of regularization used.</li>

<center> <b> Reducing Overfitting  </b> </center>  

<li>Reduce the model complexity. It could be that the model is too complex and a simpler model is required.</li>
<li>Increase the amount of training data</li>
<li>Perform feature selection: Reduce the number of features</li>
<li>Increase the amount of regularization</li>
<li>Early stopping in the training</li>
<li>Perform cross validation</li>
<li>Use ensemble methods</li>  
</br>  

* * *

<center> <h2> 4. METHODOLOGY </h2> </center> 
<h3> 4.1 Dataset </h3>   
The data used in this analysis was obtained from [Kaggle](https://www.kaggle.com/datasets/avikpaul4u/vehicle-loan-default-prediction/data). The csv files containing the data was downloaded from kaggle and placed in [github](https://github.com/chinedu2301/data698-analytics-project/tree/main/data). There are three files which are: train dataset, test dataset, and dictionary. The dictionary file explains what the variables represent.  

<h5>Variables</h5>  
There are about 41 variables in the dataset. forty (40) of the variables are predictor variables while there is one response variable. The variables in the dataset are:  
<li> <b>UniqueID</b>: Identifier for customer </li>
<li> <b>loan_default</b>: Payment default in the first EMI on due date </li>
<li> <b>disbursed_amount</b>: Amount of loan disbursed </li>
<li> <b>asset_cost</b>: Cost of the Asset </li>
<li> <b>ltv</b>: Loan to Value of asset </li>
<li> <b>branch_id</b>: Branch where the loan was disbursed </li>
<li> <b>supplier_id</b>: Vehicle Dealer where the loan was disbursed </li>
<li> <b>manufacturer_id</b>: Vehicle manufacturer (Hero, Honda, TVS, etc.) </li>
<li> <b>Current_pincode</b>: Current pincode of the customer </li>
<li> <b>Date.of.Birth</b>: Date of Birth of the customer</li>
<li> <b>Employment.Type</b>: Employment Type of the customer (Salaried/Self Employed) </li>
<li> <b>DisbursalDate</b>: Date of loan disbursement </li>
<li> <b>State_ID</b>: State of disbursement</li>
<li> <b>MobileNo_Avl_Flag</b>: If Mobile no. was shared by the customer then flagged as 1 </li>
<li> <b>Aadhar</b>: If aadhar was shared by the customer then flagged as 1 </li>
<li> <b>PAN_flag</b>: If pan was shared by the customer then flagged as 1</li>
<li> <b>VoterID_flag</b>: If voter Id was shared by the customer then flagged as 1</li>
<li> <b>Driving_flag</b>: If Driver license was shared by the customer then flagged as 1 </li>
<li> <b>Passport_flag</b>: If passport was shared by the customer then flagged as 1 </li>
<li> <b>PERFORM_CNS.SCORE</b>: Bureau Score</li>
<li> <b>PERFORM_CNS.SCORE.DESCRIPTION</b>: Bureau Score description</li>
<li> <b>PRI.NO.OF.ACCTS</b>: Count of total loans taken by the customer at the time of disbursement</li>
<li> <b>PRI.ACTIVE.ACCTS</b>: Count of active loans taken by the customer at the time of disbursement</li>
<li> <b>PRI.OVERDUE.ACCTS</b>: Count of default accounts at the time of disbursement</li>
<li> <b>PRI.CURRENT.BALANCE</b>: Total principal outstanding amount of the active loans at the time of disbursement</li>
<li> <b>PRI.SANCTIONED.AMOUNT</b>: Total amount that was sanctioned for all the loans at the time of disbursement</li>
<li> <b>PRI.DISBURSED.AMOUNT</b>: Total amount that was disbursed for all the loans at the time of disbursement</li>
<li> <b>SEC.NO.OF.ACCTS</b>: Count of total loans taken by the customer at the time of disbursement</li>
<li> <b>SEC.ACTIVE.ACCTS</b>: Count of active loans taken by the customer at the time of disbursement</li>
<li> <b>SEC.OVERDUE.ACCTS</b>: Count of default accounts at the time of disbursement</li>
<li> <b>SEC.CURRENT.BALANCE</b>: Total principal outstanding amount of the active loans at the time of disbursement</li>
<li> <b>SEC.SANCTIONED.AMOUNT</b>: Total amount that was sanctioned for all the loans at the time of disbursement</li>
<li> <b>SEC.DISBURSED.AMOUNT</b>: Total amount that was disbursed for all the loans at the time of disbursement</li>
<li> <b>PRIMARY.INSTAL.AMT</b>: EMI Amount of the primary loan</li>
<li> <b>SEC.INSTAL.AMT</b>: EMI Amount of the secondary loan</li>
<li> <b>NEW.ACCTS.IN.LAST.SIX.MONTHS</b>: New loans taken by the customer in the last 6 months before the disbursement</li>
<li> <b>DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS</b>: Loans defaulted in the last 6 months</li>
<li> <b>AVERAGE.ACCT.AGE</b>: Average loan tenure</li>
<li> <b>CREDIT.HISTORY.LENGTH</b>: Time since first loan</li>
<li> <b>NO.OF_INQUIRIES</b>: Enquiries done by the customer for loans</li>  
Note: Primary accounts are those which the customer has taken for his personal use while secondary accounts are those which the customer act as a co-applicant or guarantor.  
The response variable is the <b>loan_default</b> variable and it is a two level categorical variable with a value of 1 fir default and value 0 for no default.


<h3> 4.2 Data Cleaning and Pre-processing </h3> 
The data cleaning and pre-processing aspect of the analysis mainly involved creating derived field such as the age of the loan applicant, renaming the columns to standardize the naming conventions, adjusting the datatype of the columns, and selecting columns that would be used in the modeling/training of the data. 
The train and test dataset are already separate to start with and since the test data will be used in testing the results of the model, it too would need to be cleaned and pre-processed. To avoid extra work and errors, the functional approach has been adopted. The analysis is broken down into different sections and a single R-function is responsible for each section of the analysis. The sections are: data cleaning and pre-processing; modeling/training; and testing. Hence, there are 3 R functions: data_cleaning_preprocess; modeling_training; and predict_testing that handle each sections respectively. That way, when it is time to analyze new data, the new data is simply passed into those set of functions to achieve the desired results. Although, there is a section for exploratory data analysis, there is no function that does this.
Also, because the data is not balanced (unequal number of observations for values 1 and 0 for the response variable), the dataset was enhanced to ensure equality of the two levels to prevent the model from correctly predicting one class better 
 

* * *  

<center> <h2> 5. ANALYSIS, TESTING AND RESULTS </h2> </center> 
<h3> 5.1 Analysis </h3>  
<h4> 5.1.1 Data Cleaning and Pre-processing </h4>  
**Libraries**  
Load the required libraries for the analysis
```{r load-libraries, warning=FALSE, message=FALSE}
library(tidyverse)
library(kableExtra)
library(Amelia) # To visualize missing data
library(cowplot) # To combine plots in a grid
```

Read the datasets into memory from the github location.
```{r read-data, warning=FALSE, message=FALSE}
url_vehicle_loan_default_train = "https://raw.githubusercontent.com/chinedu2301/data698-analytics-project/main/data/vehicle_default_train_data.csv"
url_vehicle_loan_default_test = "https://raw.githubusercontent.com/chinedu2301/data698-analytics-project/main/data/vehicle_default_test_data.csv"
vehicle_loan_default_train_raw = read_csv(url_vehicle_loan_default_train) %>% as_tibble()
vehicle_loan_default_test_raw = read_csv(url_vehicle_loan_default_test) %>% as_tibble()
```

Display a few records of the dataset to have an idea of how the data looks like.
```{r display-head}
# display a few records of the raw data
raw_data_few_records <- kable(head(vehicle_loan_default_train_raw, 50), "html") %>%
                        kable_paper("hover", full_width = F) %>%
                        scroll_box(width = "850px", height = "350px")
raw_data_few_records
```

<br>
<h4>data_cleaning_preprocess function</h4>  
The function does cleaning of the data. It only accepts a dataframe (or a tibble) 
```{r data_cleaning_preprocess, warning=FALSE, message=FALSE}
data_cleaning_preprocess <- function(df){
  # This function accepts a dataframe (df) as input and returns another dataframe (cleaned_df) that is clean.
  tryCatch({
  if(is_tibble(df) | is.data.frame(df)){
    print("The dataframe is a tibble, will proceed to clean data")
    print("Data Cleaning in progress...")
    # rename all the columns in the dataframe to lowercase
    cleaned_df <- df %>% rename_all(tolower) %>%  
                # compute the age of the applicant in number of years
                mutate(date_of_birth = dmy(date_of_birth),  
                      disbursal_date = dmy(disbursal_date), 
                      age = difftime(disbursal_date, date_of_birth, units = "days"),
                      age_years = round(as.numeric(age / 365.25), 0)) %>%
               # extract the years and month component of the average_acct_age  and convert to years
                mutate(average_acct_age_year_comp = as.numeric(str_extract(average_acct_age, "\\d+")),
                      average_acct_age_mon_comp = as.numeric(str_extract(average_acct_age, "\\d+(?=mon)")),
                      average_acct_age = round((average_acct_age_year_comp + average_acct_age_mon_comp/12), 0)
                       ) %>%
                # extract the years and month component of the credit_history_length and convert to years
                mutate(credit_history_length_year_comp = as.numeric(str_extract(credit_history_length, "\\d+")),
                       credit_history_length_comp = as.numeric(str_extract(credit_history_length, "\\d+(?=mon)")),
                       credit_history_length = round((credit_history_length_year_comp + credit_history_length_comp/12), 0)
                       )  %>%
                # clean up the perform_cns_score_distribution to include only a few categories
                mutate(lowercase_cns_description = tolower(perform_cns_score_description),
                       perform_cns_score_description = case_when(
                                str_detect(lowercase_cns_description, "very low risk") ~ "very_low_risk",
                                str_detect(lowercase_cns_description, "low risk") ~ "low_risk",
                                str_detect(lowercase_cns_description, "medium risk") ~ "medium_risk",
                                str_detect(lowercase_cns_description, "high risk") ~ "high_risk",
                                str_detect(lowercase_cns_description, "very high risk") ~ "very_high_risk",
                                str_detect(lowercase_cns_description, "not scored|no bureau") ~ "low_risk",
                                TRUE ~ "none"))  %>%
                # clean up the employment type to have only few categories
                mutate(lower_case_employment_type = tolower(employment_type),
                       employment_type = case_when(
                                str_detect(lower_case_employment_type, "salaried") ~ "salaried",
                                str_detect(lower_case_employment_type, "self employed") ~ "self_employed",
                                TRUE ~ "not_reported")) %>%
                # select only the required columns
                select(
                  age_years, disbursed_amount, asset_cost, ltv, manufacturer_id, employment_type, perform_cns_score_description,
                  pri_no_of_accts, pri_active_accts, pri_overdue_accts, pri_current_balance, pri_sanctioned_amount,
                  pri_disbursed_amount, sec_no_of_accts, sec_active_accts, sec_overdue_accts, sec_current_balance,
                  sec_sanctioned_amount, sec_disbursed_amount, primary_instal_amt, sec_instal_amt, new_accts_in_last_six_months,
                  delinquent_accts_in_last_six_months, average_acct_age, credit_history_length, no_of_inquiries, loan_default
                )
                print("Data Cleaning complete!!!")
    return(cleaned_df)
    
  }
  else{
  print("The dataframe is not a tibble. Kindly have your data in the form of a dataframe or a tibble")
  }
    
  },
  #if an error occurs, tell me the error
  error=function(e) {
        message('An Error Occurred')
        print(e)
        },
  #or if a warning occurs, tell me the warning
  warning=function(w) {
        message('A Warning Occurred')
        print(w)
        return(NA)
        }
    )
  
}
```

Use the data_cleaning_preprocess function to clean the data.

```{r}
# clean the data
vehicle_loan_default_train_cleaned = data_cleaning_preprocess(vehicle_loan_default_train_raw)
```
 Display a few records of the cleaned data.

```{r}
# display a few records of the cleaned data
cleaned_data_few_records <- kable(head(vehicle_loan_default_train_cleaned, 200), "html") %>% 
                            kable_paper("hover", full_width = F) %>%
                            scroll_box(width = "850px", height = "350px")
cleaned_data_few_records
```

<br>
<h4> 5.1.2 Exploratory Data Analysis </h4>
<h4> 5.2.2 Model Training </h4>
<h5> 5.2.2.1 Model Training - Logistic Regression </h5>
<h5> 5.2.2.2 Model Training - Random Forest </h5>
<h5> 5.2.2.3 Model Training - XGBoost </h5>
<h5> 5.2.2.4 Model Training - Support Vector Machines </h5>
<h3> 5.3 Testing </h3>
<h3> 5.4 Results </h3>


</br>

***

<center> <h2> 6. CONCLUSION </h2> </center> 


</br>  

***
<center> <h2> 7. REFERENCES </h2> </center>  
Agarwal, S., Ambrose, B. W., & Chomsisengphet, S. (2008). Determinants of automobile loan default and prepayment. Economic Perspectives - Federal Reserve Bank of Chicago.  

Altman, E. I., & Saunders, A. (1998). Credit risk measurement: Developments over the last
20 years. Journal of Banking and Finance, 21(11-12), 1728–1742.

Agrawal, A., Agrawal, M., & Raizada, D. A. (2014). Predicting defaults in commercial vehicle
loans using logistic regression: Case of an indian nbfc. International Journal of Research in Commerce and Management, 5, 22–28.  

Brownlee, J. (2019, August 12). Overfitting and Underfitting With Machine Learning Algorithms. Machine Learning Mastery. Retrieved November 4, 2023, from https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/#:%7E:text=Overfitting%3A%20Good%20performance%20on%20the,poor%20generalization%20to%20other%20data

Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785-794).  

Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine learning, 20(3), 273-297.  

Crook, J. N., Edelman, D. B., & Thomas, L. C. (2007). Recent developments in consumer credit risk assessment. European Journal of Operational Research, 183, 447–1465.  

Cromer, O. C., Purdy, K. W., Cromer, G. C., & Foster, C. G. (2023, October 13). Automobile | Definition, History, industry, design, & Facts. Encyclopedia Britannica. https://www.britannica.com/technology/automobile  

Diez, D., Barr, C. D., & Cetinkaya-Rundel, M. (2019). OpenIntro statistics 4th Edition.  

Education, I. C. (2021, March 25). Underfitting. IBM Cloud Learn. Retrieved November 5, 2023, from https://www.ibm.com/cloud/learn/underfitting  

Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232.  

GeeksforGeeks. (2021, October 20). ML | Underfitting and Overfitting. Retrieved November 4, 2023, from https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/  

Hand, D.J. and Henley, W.E. (1997) Statistical Classification Methods in Consumer Credit Scoring: A Review. Journal of Royal Statistical Society, 160, 523-541.  

Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.  

Hao, C., Alam, M. M., & Carling, K. (2010). Review of the literature on credit risk modeling: Development of the past 10 years. Banks and Bank Systems, 5 (3).

Lessmann, S., Baesens, B., Seow, H.-V., & Thomas, L. C. (2015). Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research. European Journal of Operational Research, 247 (1), 124–136.  

Martin, A. (2023). What is an auto loan? Bankrate. https://www.bankrate.com/loans/auto-loans/what-is-an-auto-loan/  

Model Fit: Underfitting vs. Overfitting - Amazon Machine Learning. (n.d.). Amazon Machine Learning Developer Guide. Retrieved November 4, 2023, from https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html

O’Brien, S. (2023, February 4). Auto loan delinquencies are rising. Here’s what to do if you’re struggling with payments. CNBC. https://www.cnbc.com/2023/02/04/auto-loan-delinquencies-rise-what-to-do-if-you-struggle-with-payments.html  

Probasco, J. (2023). Expert explanation of how auto loans work. Investopedia. https://www.investopedia.com/how-car-loans-work-5202265  

U.S. vehicle fleet 1990-2021 | Statista. (2023, August 24). Statista. https://www.statista.com/statistics/183505/number-of-vehicles-in-the-united-states-since-1990/  

U.S.: average selling price of new vehicles 2022 | Statista. (2023, June 7). Statista. https://www.statista.com/statistics/274927/new-vehicle-average-selling-price-in-the-united-states/  

Witkowski, R. (2023, June 2). For People Under 30, Car Loan Delinquencies Hit A 15-Year High. Is The Economy Running Out Of Gas? Forbes Advisor. https://www.forbes.com/advisor/auto-loans/car-loan-late-payments/  






